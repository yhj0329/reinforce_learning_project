{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import flatten, conv2d, fully_connected\n",
    "from collections import deque, Counter\n",
    "import random\n",
    "from datetime import datetime\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from hide_and_seek_3thief import HideAndSeekEnv\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o - - - - - - - - - - - - - - - - - o\n",
      "| P |     3 |           |           |\n",
      "|           | - |   |   | - | - |   |\n",
      "|   |               |   |           |\n",
      "| - |                       | - |   |\n",
      "|       |   |                       |\n",
      "|   | 2                     | - |   |\n",
      "|   |         1             |       |\n",
      "|   | - | - | - |           | - |   |\n",
      "|                               | - |\n",
      "o - - - - - - - - - - - - - - - - - o\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 환경 설정\n",
    "env = HideAndSeekEnv()\n",
    "n_outputs = env.action_space.n\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q 네트워크를 구축하기 위해 q_network라는 함수를 정의\n",
    "# Q 네트워크를 입력하고 해당 상태의 모든 작업에 대한 Q 값을 얻는다.\n",
    "# fully connected layer가 이어지는 동일한 패딩을 가진 3개의 convolutional layers로 Q 네트워크를 구축\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def q_network(X, name_scope):\n",
    "    #layers 초기화\n",
    "    initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "    \n",
    "    with tf.variable_scope(name_scope) as scope:\n",
    "        #convolutional layers 초기화\n",
    "        layer_1 = conv2d(X, num_outputs=32, kernel_size=(5,5), stride=4, padding=\"SAME\", weights_initializer=initializer)\n",
    "        tf.summary.histogram('layer_1',layer_1)\n",
    "        \n",
    "        layer_2 = conv2d(layer_1, num_outputs=64, kernel_size=(3,3), stride=2, padding=\"SAME\", weights_initializer=initializer)\n",
    "        tf.summary.histogram('layer_2',layer_2)\n",
    "        \n",
    "        layer_3 = conv2d(layer_2, num_outputs=64, kernel_size=(3,3), stride=1, padding=\"SAME\", weights_initializer=initializer)\n",
    "        tf.summary.histogram('layer_3',layer_3)\n",
    "        \n",
    "        # fully connected layer에 공급하기 전에 layer_3의 결과를 평탄화\n",
    "        flat = flatten(layer_3)\n",
    "        \n",
    "        fc = fully_connected(flat, num_outputs=128, weights_initializer=initializer)\n",
    "        tf.summary.histogram('fc',fc)\n",
    "        \n",
    "        output = fully_connected(fc, num_outputs=n_outputs, activation_fn=None, weights_initializer=initializer)\n",
    "        tf.summary.histogram('output',output)\n",
    "        \n",
    "        #vars는 가중치와 같은 네트워크 매개변수를 저장\n",
    "        vars = {v.name[len(scope.name):]: v for v in tf.get_collection(key=tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope.name)}\n",
    "        return vars, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 엡실론 그리디 정책을 수행하기 위해 epsilon_greedy라는 함수를 정의\n",
    "# 영원히 탐색하고 싶지 않기 때문에 엡실론의 가치가 시간이 지남에 따라 쇠퇴하는 쇠퇴 엡실론 탐욕 정책을 사용\n",
    "# 즉, 시간이 지남에 따라 우리 정책은 좋은 행동만 이용할 것입니다.\n",
    "eps_min=0.05\n",
    "eps_max=0.5\n",
    "eps_decay_steps = 500000\n",
    "\n",
    "def epsilon_greedy(action, step):\n",
    "    p = np.random.random(1).squeeze()\n",
    "    epsilon = max(eps_min, eps_max - (eps_max-eps_min) * step/eps_decay_steps)\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(n_outputs)\n",
    "    else:\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경험을 보유하는 50000의 경험 버퍼를 초기화\n",
    "# 에이전트의 모든 경험, 즉 (상태, 행동, 보상)을 경험 버퍼에 저장하고 네트워크 훈련을 위해 이 경험의 미니배치에서 샘플링\n",
    "buffer_len = 500000\n",
    "exp_buffer = deque(maxlen=buffer_len)\n",
    "\n",
    "# 메모리에서 경험을 샘플링하기 위해 sampled_memories라는 함수를 정의\n",
    "# 배치 크기는 메모리에서 샘플링된 경험의 수이다.\n",
    "def sample_memories(batch_size):\n",
    "    perm_batch = np.random.permutation(len(exp_buffer))[:batch_size]\n",
    "    mem = np.array(exp_buffer)[perm_batch]\n",
    "    return mem[:,0],mem[:,1],mem[:,2],mem[:,3],mem[:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 정의\n",
    "num_episodes = 1000\n",
    "batch_size = 48\n",
    "input_shape = (None, 11, 19, 1)\n",
    "learning_rate = 0.001\n",
    "X_shape = (None, 11, 19, 1)\n",
    "discount_factor = 0.97\n",
    "\n",
    "global_step = 0\n",
    "copy_steps = 100\n",
    "steps_train = 4\n",
    "start_steps = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = 'ch8_logs'\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# 입력에 대한 게임 상태를 정의\n",
    "X = tf.placeholder(tf.float32, shape=X_shape)\n",
    "\n",
    "# 교육을 토글하기 위해 in_training_model이라는 부울을 정의\n",
    "in_training_mode = tf.placeholder(tf.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 대상 Q 네트워크를 구축\n",
    "# 입력 X를 취하고 상태의 모든 작업에 대해 Q 값을 생성하는 Q 네트워크를 구축\n",
    "mainQ, mainQ_outputs = q_network(X, 'mainQ')\n",
    "\n",
    "# 목표 Q 네트워크를 구축\n",
    "targetQ, targetQ_outputs = q_network(X, 'targetQ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 행동 값에 대한 자리 표시자 정의\n",
    "X_action = tf.placeholder(tf.int32, shape=(None,))\n",
    "Q_action = tf.reduce_sum(targetQ_outputs * tf.one_hot(X_action, n_outputs), axis=-1,keep_dims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 Q 네트워크 매개변수를 대상 Q 네트워크에 복사\n",
    "copy_op = [tf.assign(main_name, targetQ[var_name]) for var_name, main_name in mainQ.items()]\n",
    "copy_target_to_main = tf.group(*copy_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent optimizer를 사용하여 손실 계산 및 최적화\n",
    "\n",
    "# 행동에 대한 자리 표시자를 정의\n",
    "y = tf.placeholder(tf.float32, shape=(None,1))\n",
    "\n",
    "# 실제 값과 예측 값의 차이인 손실을 계산\n",
    "loss = tf.reduce_mean(tf.square(y - Q_action))\n",
    "\n",
    "# loss을 최소화하기 위해 adam optimizer를 사용\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "loss_summary = tf.summary.scalar('Loss', loss)\n",
    "merge_summary = tf.summary.merge_all()\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4481 Reward -31448\n",
      "Epoch 1205 Reward -7256\n",
      "Epoch 1625 Reward -7595\n",
      "Epoch 705 Reward -2814\n",
      "Epoch 947 Reward -2939\n",
      "Epoch 2511 Reward -8742\n",
      "Epoch 1022 Reward -2807\n",
      "Epoch 1147 Reward -2842\n",
      "Epoch 738 Reward -1983\n",
      "Epoch 319 Reward -709\n",
      "Epoch 2072 Reward -6035\n",
      "Epoch 1150 Reward -3097\n",
      "Epoch 302 Reward -809\n",
      "Epoch 1489 Reward -4255\n",
      "Epoch 1405 Reward -4630\n",
      "Epoch 360 Reward -939\n",
      "Epoch 1701 Reward -4764\n",
      "Epoch 833 Reward -2222\n",
      "Epoch 580 Reward -1537\n",
      "Epoch 1870 Reward -5680\n",
      "Epoch 1113 Reward -3159\n",
      "Epoch 541 Reward -1219\n",
      "Epoch 529 Reward -1468\n",
      "Epoch 858 Reward -2211\n",
      "Epoch 850 Reward -2293\n",
      "Epoch 1025 Reward -2675\n",
      "Epoch 876 Reward -2580\n",
      "Epoch 563 Reward -1340\n",
      "Epoch 2531 Reward -6845\n",
      "Epoch 978 Reward -2772\n",
      "Epoch 1254 Reward -3543\n",
      "Epoch 293 Reward -665\n",
      "Epoch 1144 Reward -3496\n",
      "Epoch 528 Reward -1215\n",
      "Epoch 922 Reward -2500\n",
      "Epoch 1061 Reward -2450\n",
      "Epoch 2082 Reward -5685\n",
      "Epoch 249 Reward -486\n",
      "Epoch 145 Reward -158\n",
      "Epoch 2494 Reward -6043\n",
      "Epoch 1667 Reward -4208\n",
      "Epoch 1315 Reward -3631\n",
      "Epoch 848 Reward -2031\n",
      "Epoch 469 Reward -1048\n",
      "Epoch 1441 Reward -3622\n",
      "Epoch 1058 Reward -2717\n",
      "Epoch 333 Reward -624\n",
      "Epoch 830 Reward -2417\n",
      "Epoch 160 Reward -442\n",
      "Epoch 540 Reward -1200\n",
      "Epoch 652 Reward -1753\n",
      "Epoch 1090 Reward -2570\n",
      "Epoch 199 Reward -364\n",
      "Epoch 1743 Reward -4653\n",
      "Epoch 520 Reward -1522\n",
      "Epoch 814 Reward -2311\n",
      "Epoch 696 Reward -1581\n",
      "Epoch 267 Reward -495\n",
      "Epoch 1086 Reward -2746\n",
      "Epoch 1621 Reward -3883\n",
      "Epoch 1352 Reward -3443\n",
      "Epoch 630 Reward -1254\n",
      "Epoch 380 Reward -941\n",
      "Epoch 288 Reward -732\n",
      "Epoch 791 Reward -1775\n",
      "Epoch 587 Reward -1571\n",
      "Epoch 313 Reward -721\n",
      "Epoch 304 Reward -613\n",
      "Epoch 1103 Reward -3005\n",
      "Epoch 896 Reward -2276\n",
      "Epoch 908 Reward -2144\n",
      "Epoch 884 Reward -1850\n",
      "Epoch 1970 Reward -4646\n",
      "Epoch 532 Reward -1597\n",
      "Epoch 973 Reward -2038\n",
      "Epoch 1156 Reward -2671\n",
      "Epoch 1345 Reward -3760\n",
      "Epoch 758 Reward -1832\n",
      "Epoch 135 Reward -570\n",
      "Epoch 507 Reward -1221\n",
      "Epoch 828 Reward -2154\n",
      "Epoch 133 Reward -145\n",
      "Epoch 278 Reward -632\n",
      "Epoch 501 Reward -1458\n",
      "Epoch 1086 Reward -2970\n",
      "Epoch 646 Reward -1297\n",
      "Epoch 802 Reward -1669\n",
      "Epoch 949 Reward -2338\n",
      "Epoch 103 Reward -52\n",
      "Epoch 558 Reward -1155\n",
      "Epoch 1201 Reward -2851\n",
      "Epoch 778 Reward -1645\n",
      "Epoch 548 Reward -1235\n",
      "Epoch 965 Reward -2138\n",
      "Epoch 679 Reward -1402\n",
      "Epoch 1283 Reward -3033\n",
      "Epoch 695 Reward -1562\n",
      "Epoch 1303 Reward -3673\n",
      "Epoch 1240 Reward -3016\n",
      "Epoch 1577 Reward -3146\n",
      "Epoch 360 Reward -795\n",
      "Epoch 600 Reward -1845\n",
      "Epoch 176 Reward -296\n",
      "Epoch 482 Reward -890\n",
      "Epoch 355 Reward -1123\n",
      "Epoch 96 Reward 45\n",
      "Epoch 1118 Reward -2174\n",
      "Epoch 337 Reward -439\n",
      "Epoch 403 Reward -1000\n",
      "Epoch 535 Reward -1294\n",
      "Epoch 1067 Reward -2942\n",
      "Epoch 939 Reward -1941\n",
      "Epoch 758 Reward -1490\n",
      "Epoch 734 Reward -1529\n",
      "Epoch 265 Reward -709\n",
      "Epoch 409 Reward -457\n",
      "Epoch 399 Reward -681\n",
      "Epoch 1391 Reward -3095\n",
      "Epoch 639 Reward -1569\n",
      "Epoch 520 Reward -1342\n",
      "Epoch 513 Reward -1299\n",
      "Epoch 693 Reward -1587\n",
      "Epoch 449 Reward -1487\n",
      "Epoch 211 Reward -358\n",
      "Epoch 1877 Reward -5039\n",
      "Epoch 490 Reward -988\n",
      "Epoch 822 Reward -2283\n",
      "Epoch 944 Reward -2072\n",
      "Epoch 233 Reward -299\n",
      "Epoch 506 Reward -1148\n",
      "Epoch 952 Reward -2197\n",
      "Epoch 84 Reward 57\n",
      "Epoch 826 Reward -2134\n",
      "Epoch 311 Reward -665\n",
      "Epoch 731 Reward -1931\n",
      "Epoch 326 Reward -626\n",
      "Epoch 641 Reward -1671\n",
      "Epoch 943 Reward -2053\n",
      "Epoch 480 Reward -1059\n",
      "Epoch 513 Reward -1002\n",
      "Epoch 443 Reward -1616\n",
      "Epoch 563 Reward -1205\n",
      "Epoch 34 Reward 89\n",
      "Epoch 313 Reward -514\n",
      "Epoch 261 Reward -750\n",
      "Epoch 100 Reward -175\n",
      "Epoch 297 Reward -894\n",
      "Epoch 756 Reward -2064\n",
      "Epoch 579 Reward -1914\n",
      "Epoch 1002 Reward -2337\n",
      "Epoch 437 Reward -935\n",
      "Epoch 420 Reward -747\n",
      "Epoch 787 Reward -1591\n",
      "Epoch 1275 Reward -2772\n",
      "Epoch 688 Reward -1564\n",
      "Epoch 744 Reward -1746\n",
      "Epoch 900 Reward -2064\n",
      "Epoch 490 Reward -1267\n",
      "Epoch 502 Reward -1405\n",
      "Epoch 165 Reward -150\n",
      "Epoch 295 Reward -505\n",
      "Epoch 1845 Reward -4179\n",
      "Epoch 1072 Reward -2254\n",
      "Epoch 694 Reward -1822\n",
      "Epoch 388 Reward -760\n",
      "Epoch 207 Reward -273\n",
      "Epoch 328 Reward -646\n",
      "Epoch 593 Reward -1415\n",
      "Epoch 739 Reward -1723\n",
      "Epoch 302 Reward -674\n",
      "Epoch 961 Reward -2215\n",
      "Epoch 712 Reward -1687\n",
      "Epoch 831 Reward -2112\n",
      "Epoch 257 Reward -467\n",
      "Epoch 1106 Reward -2990\n",
      "Epoch 189 Reward -318\n",
      "Epoch 481 Reward -1681\n",
      "Epoch 489 Reward -1293\n",
      "Epoch 567 Reward -1596\n",
      "Epoch 849 Reward -2814\n",
      "Epoch 640 Reward -1831\n",
      "Epoch 571 Reward -961\n",
      "Epoch 882 Reward -2577\n",
      "Epoch 408 Reward -1329\n",
      "Epoch 883 Reward -1570\n",
      "Epoch 326 Reward -743\n",
      "Epoch 560 Reward -1409\n",
      "Epoch 428 Reward -989\n",
      "Epoch 280 Reward -481\n",
      "Epoch 438 Reward -1080\n",
      "Epoch 1182 Reward -2472\n",
      "Epoch 408 Reward -870\n",
      "Epoch 332 Reward -767\n",
      "Epoch 65 Reward 75\n",
      "Epoch 703 Reward -1723\n",
      "Epoch 193 Reward -205\n",
      "Epoch 198 Reward -300\n",
      "Epoch 525 Reward -1167\n",
      "Epoch 783 Reward -1758\n",
      "Epoch 323 Reward -578\n",
      "Epoch 308 Reward -644\n",
      "Epoch 176 Reward -557\n",
      "Epoch 560 Reward -1391\n",
      "Epoch 530 Reward -983\n",
      "Epoch 62 Reward 106\n",
      "Epoch 179 Reward -227\n",
      "Epoch 230 Reward -395\n",
      "Epoch 991 Reward -1984\n",
      "Epoch 207 Reward -309\n",
      "Epoch 347 Reward -521\n",
      "Epoch 678 Reward -1221\n",
      "Epoch 487 Reward -859\n",
      "Epoch 514 Reward -1120\n",
      "Epoch 573 Reward -1134\n",
      "Epoch 288 Reward -507\n",
      "Epoch 695 Reward -1679\n",
      "Epoch 545 Reward -1043\n",
      "Epoch 307 Reward -517\n",
      "Epoch 871 Reward -1909\n",
      "Epoch 186 Reward -225\n",
      "Epoch 344 Reward -707\n",
      "Epoch 102 Reward -159\n",
      "Epoch 534 Reward -1095\n",
      "Epoch 1138 Reward -2113\n",
      "Epoch 373 Reward -916\n",
      "Epoch 99 Reward 33\n",
      "Epoch 203 Reward -305\n",
      "Epoch 1430 Reward -3620\n",
      "Epoch 801 Reward -2001\n",
      "Epoch 688 Reward -1357\n",
      "Epoch 733 Reward -1825\n",
      "Epoch 524 Reward -1292\n",
      "Epoch 540 Reward -1299\n",
      "Epoch 2563 Reward -6823\n",
      "Epoch 695 Reward -2075\n",
      "Epoch 881 Reward -2126\n",
      "Epoch 411 Reward -1062\n",
      "Epoch 493 Reward -910\n",
      "Epoch 1205 Reward -2918\n",
      "Epoch 687 Reward -1275\n",
      "Epoch 271 Reward -490\n",
      "Epoch 471 Reward -1032\n",
      "Epoch 252 Reward -669\n",
      "Epoch 205 Reward -424\n",
      "Epoch 108 Reward -129\n",
      "Epoch 1173 Reward -2571\n",
      "Epoch 549 Reward -1956\n",
      "Epoch 395 Reward -812\n",
      "Epoch 873 Reward -1947\n",
      "Epoch 236 Reward -833\n",
      "Epoch 165 Reward -177\n",
      "Epoch 327 Reward -546\n",
      "Epoch 625 Reward -997\n",
      "Epoch 847 Reward -1993\n",
      "Epoch 785 Reward -1832\n",
      "Epoch 168 Reward -351\n",
      "Epoch 457 Reward -973\n",
      "Epoch 240 Reward -531\n",
      "Epoch 616 Reward -1447\n",
      "Epoch 429 Reward -1638\n",
      "Epoch 754 Reward -1522\n",
      "Epoch 1697 Reward -4301\n",
      "Epoch 231 Reward -441\n",
      "Epoch 1533 Reward -3795\n",
      "Epoch 416 Reward -1085\n",
      "Epoch 289 Reward -463\n",
      "Epoch 389 Reward -959\n",
      "Epoch 910 Reward -1912\n",
      "Epoch 140 Reward -395\n",
      "Epoch 688 Reward -1402\n",
      "Epoch 364 Reward -592\n",
      "Epoch 462 Reward -663\n",
      "Epoch 780 Reward -2070\n",
      "Epoch 279 Reward -1173\n",
      "Epoch 193 Reward -583\n",
      "Epoch 151 Reward -145\n",
      "Epoch 754 Reward -1684\n",
      "Epoch 222 Reward -720\n",
      "Epoch 1884 Reward -4893\n",
      "Epoch 606 Reward -1194\n",
      "Epoch 1104 Reward -2583\n",
      "Epoch 859 Reward -2131\n",
      "Epoch 517 Reward -1168\n",
      "Epoch 611 Reward -1451\n",
      "Epoch 736 Reward -1909\n",
      "Epoch 514 Reward -1084\n",
      "Epoch 339 Reward -765\n",
      "Epoch 905 Reward -1844\n",
      "Epoch 914 Reward -1934\n",
      "Epoch 935 Reward -2063\n",
      "Epoch 217 Reward -274\n",
      "Epoch 1045 Reward -2047\n",
      "Epoch 763 Reward -1450\n",
      "Epoch 996 Reward -1908\n",
      "Epoch 392 Reward -719\n",
      "Epoch 579 Reward -1464\n",
      "Epoch 308 Reward -806\n",
      "Epoch 474 Reward -855\n",
      "Epoch 648 Reward -1200\n",
      "Epoch 1195 Reward -3016\n",
      "Epoch 394 Reward -721\n",
      "Epoch 423 Reward -660\n",
      "Epoch 638 Reward -1181\n",
      "Epoch 421 Reward -596\n",
      "Epoch 336 Reward -708\n",
      "Epoch 472 Reward -655\n",
      "Epoch 734 Reward -1682\n",
      "Epoch 282 Reward -501\n",
      "Epoch 962 Reward -2270\n",
      "Epoch 1254 Reward -2841\n",
      "Epoch 744 Reward -1701\n",
      "Epoch 623 Reward -1625\n",
      "Epoch 1829 Reward -3848\n",
      "Epoch 607 Reward -1654\n",
      "Epoch 915 Reward -2115\n",
      "Epoch 306 Reward -489\n",
      "Epoch 397 Reward -1003\n",
      "Epoch 175 Reward -196\n",
      "Epoch 495 Reward -1038\n",
      "Epoch 154 Reward -274\n",
      "Epoch 239 Reward -252\n",
      "Epoch 472 Reward -745\n",
      "Epoch 1853 Reward -4898\n",
      "Epoch 1238 Reward -2402\n",
      "Epoch 659 Reward -1517\n",
      "Epoch 1052 Reward -2153\n",
      "Epoch 573 Reward -1297\n",
      "Epoch 389 Reward -599\n",
      "Epoch 802 Reward -2308\n",
      "Epoch 500 Reward -791\n",
      "Epoch 415 Reward -769\n",
      "Epoch 525 Reward -924\n",
      "Epoch 567 Reward -1191\n",
      "Epoch 369 Reward -849\n",
      "Epoch 709 Reward -1279\n",
      "Epoch 177 Reward -91\n",
      "Epoch 909 Reward -1848\n",
      "Epoch 322 Reward -541\n",
      "Epoch 638 Reward -992\n",
      "Epoch 378 Reward -660\n",
      "Epoch 550 Reward -1102\n",
      "Epoch 347 Reward -710\n",
      "Epoch 660 Reward -1473\n",
      "Epoch 150 Reward -189\n",
      "Epoch 214 Reward -991\n",
      "Epoch 428 Reward -2024\n",
      "Epoch 1155 Reward -2310\n",
      "Epoch 1130 Reward -3185\n",
      "Epoch 615 Reward -1203\n",
      "Epoch 215 Reward -218\n",
      "Epoch 285 Reward -333\n",
      "Epoch 602 Reward -1487\n",
      "Epoch 1902 Reward -5100\n",
      "Epoch 300 Reward -573\n",
      "Epoch 639 Reward -1353\n",
      "Epoch 831 Reward -1518\n",
      "Epoch 85 Reward -34\n",
      "Epoch 882 Reward -1677\n",
      "Epoch 392 Reward -701\n",
      "Epoch 480 Reward -1113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 732 Reward -1905\n",
      "Epoch 1717 Reward -4168\n",
      "Epoch 476 Reward -1505\n",
      "Epoch 1350 Reward -2694\n",
      "Epoch 691 Reward -1423\n",
      "Epoch 343 Reward -508\n",
      "Epoch 212 Reward -323\n",
      "Epoch 1640 Reward -3416\n",
      "Epoch 634 Reward -2131\n",
      "Epoch 428 Reward -863\n",
      "Epoch 532 Reward -931\n",
      "Epoch 424 Reward -679\n",
      "Epoch 1939 Reward -4498\n",
      "Epoch 249 Reward -423\n",
      "Epoch 551 Reward -1337\n",
      "Epoch 797 Reward -2213\n",
      "Epoch 851 Reward -1502\n",
      "Epoch 301 Reward -610\n",
      "Epoch 54 Reward 105\n",
      "Epoch 1433 Reward -3596\n",
      "Epoch 241 Reward -667\n",
      "Epoch 1279 Reward -2587\n",
      "Epoch 2449 Reward -4801\n",
      "Epoch 938 Reward -2201\n",
      "Epoch 2713 Reward -5992\n",
      "Epoch 1191 Reward -2256\n",
      "Epoch 1063 Reward -1939\n",
      "Epoch 1093 Reward -1951\n",
      "Epoch 2303 Reward -5114\n",
      "Epoch 722 Reward -1454\n",
      "Epoch 1386 Reward -2685\n",
      "Epoch 1100 Reward -2075\n",
      "Epoch 1640 Reward -3146\n",
      "Epoch 1353 Reward -2616\n",
      "Epoch 1098 Reward -2064\n",
      "Epoch 1837 Reward -3217\n",
      "Epoch 1917 Reward -3990\n",
      "Epoch 1530 Reward -4404\n",
      "Epoch 749 Reward -1715\n",
      "Epoch 1167 Reward -2187\n",
      "Epoch 236 Reward -383\n",
      "Epoch 423 Reward -741\n",
      "Epoch 786 Reward -1338\n",
      "Epoch 265 Reward -340\n",
      "Epoch 327 Reward -573\n",
      "Epoch 1325 Reward -2732\n",
      "Epoch 1157 Reward -1907\n",
      "Epoch 772 Reward -1396\n",
      "Epoch 1682 Reward -3638\n",
      "Epoch 3266 Reward -6383\n",
      "Epoch 1073 Reward -2211\n",
      "Epoch 207 Reward -318\n",
      "Epoch 475 Reward -1108\n",
      "Epoch 1230 Reward -2421\n",
      "Epoch 1501 Reward -2665\n",
      "Epoch 1959 Reward -3420\n",
      "Epoch 225 Reward -363\n",
      "Epoch 1961 Reward -3755\n",
      "Epoch 591 Reward -963\n",
      "Epoch 3838 Reward -7153\n",
      "Epoch 758 Reward -1976\n",
      "Epoch 355 Reward -943\n",
      "Epoch 930 Reward -1590\n",
      "Epoch 1942 Reward -5455\n",
      "Epoch 1755 Reward -3594\n",
      "Epoch 2516 Reward -4985\n",
      "Epoch 1136 Reward -2237\n",
      "Epoch 4010 Reward -8063\n",
      "Epoch 1581 Reward -3168\n",
      "Epoch 960 Reward -1953\n",
      "Epoch 1672 Reward -3251\n",
      "Epoch 828 Reward -1335\n",
      "Epoch 3172 Reward -6640\n",
      "Epoch 198 Reward -417\n",
      "Epoch 576 Reward -1263\n",
      "Epoch 366 Reward -486\n",
      "Epoch 1012 Reward -1537\n",
      "Epoch 633 Reward -879\n",
      "Epoch 258 Reward -621\n",
      "Epoch 1915 Reward -4267\n",
      "Epoch 781 Reward -1378\n",
      "Epoch 1279 Reward -2101\n",
      "Epoch 664 Reward -929\n",
      "Epoch 1699 Reward -3295\n",
      "Epoch 1024 Reward -1909\n",
      "Epoch 1920 Reward -3741\n",
      "Epoch 83 Reward -68\n",
      "Epoch 566 Reward -1325\n",
      "Epoch 1283 Reward -2969\n",
      "Epoch 1271 Reward -2480\n",
      "Epoch 747 Reward -1074\n",
      "Epoch 903 Reward -1698\n",
      "Epoch 117 Reward -120\n",
      "Epoch 184 Reward -214\n",
      "Epoch 1033 Reward -1801\n",
      "Epoch 122 Reward -233\n",
      "Epoch 1053 Reward -2037\n",
      "Epoch 137 Reward -230\n",
      "Epoch 241 Reward -334\n",
      "Epoch 1250 Reward -3062\n",
      "Epoch 1367 Reward -2279\n",
      "Epoch 175 Reward -331\n",
      "Epoch 1389 Reward -2697\n",
      "Epoch 193 Reward -295\n",
      "Epoch 1424 Reward -2363\n",
      "Epoch 2091 Reward -4596\n",
      "Epoch 1511 Reward -2505\n",
      "Epoch 147 Reward -69\n",
      "Epoch 1458 Reward -2865\n",
      "Epoch 1457 Reward -2630\n",
      "Epoch 394 Reward -406\n",
      "Epoch 2632 Reward -6667\n",
      "Epoch 189 Reward -111\n",
      "Epoch 754 Reward -1297\n",
      "Epoch 708 Reward -1080\n",
      "Epoch 1220 Reward -2303\n",
      "Epoch 1746 Reward -3621\n",
      "Epoch 817 Reward -1576\n",
      "Epoch 141 Reward -63\n",
      "Epoch 462 Reward -717\n",
      "Epoch 3868 Reward -6805\n",
      "Epoch 1106 Reward -1820\n",
      "Epoch 2179 Reward -3541\n",
      "Epoch 734 Reward -1205\n",
      "Epoch 710 Reward -1361\n",
      "Epoch 1896 Reward -4545\n",
      "Epoch 1333 Reward -1930\n",
      "Epoch 828 Reward -1605\n",
      "Epoch 1221 Reward -1917\n",
      "Epoch 118 Reward -41\n",
      "Epoch 1206 Reward -1965\n",
      "Epoch 226 Reward -139\n",
      "Epoch 2191 Reward -3571\n",
      "Epoch 1862 Reward -3791\n",
      "Epoch 96 Reward 45\n",
      "Epoch 254 Reward -320\n",
      "Epoch 163 Reward -95\n",
      "Epoch 900 Reward -1137\n",
      "Epoch 1341 Reward -2883\n",
      "Epoch 382 Reward -1006\n",
      "Epoch 6135 Reward -9999\n",
      "Epoch 3897 Reward -6033\n",
      "Epoch 497 Reward -941\n",
      "Epoch 359 Reward -264\n",
      "Epoch 3080 Reward -4785\n",
      "Epoch 3204 Reward -5196\n",
      "Epoch 2415 Reward -3840\n",
      "Epoch 703 Reward -1561\n",
      "Epoch 129 Reward -150\n",
      "Epoch 2414 Reward -3785\n",
      "Epoch 2689 Reward -4582\n",
      "Epoch 701 Reward -1073\n",
      "Epoch 159 Reward -297\n",
      "Epoch 1901 Reward -2867\n",
      "Epoch 628 Reward -883\n",
      "Epoch 3264 Reward -5409\n",
      "Epoch 3198 Reward -5487\n",
      "Epoch 2072 Reward -3407\n",
      "Epoch 1742 Reward -5966\n",
      "Epoch 859 Reward -1196\n",
      "Epoch 2981 Reward -4424\n",
      "Epoch 2942 Reward -4097\n",
      "Epoch 1982 Reward -3074\n",
      "Epoch 1019 Reward -1562\n",
      "Epoch 2696 Reward -3671\n",
      "Epoch 743 Reward -2321\n",
      "Epoch 1028 Reward -2615\n",
      "Epoch 3123 Reward -5907\n",
      "Epoch 1680 Reward -2349\n",
      "Epoch 1204 Reward -3358\n",
      "Epoch 3487 Reward -4687\n",
      "Epoch 1134 Reward -1479\n",
      "Epoch 904 Reward -1357\n",
      "Epoch 2194 Reward -3277\n",
      "Epoch 2246 Reward -3113\n",
      "Epoch 2046 Reward -2778\n",
      "Epoch 6065 Reward -7922\n",
      "Epoch 2043 Reward -3009\n",
      "Epoch 731 Reward -842\n",
      "Epoch 233 Reward -398\n",
      "Epoch 1949 Reward -2492\n",
      "Epoch 518 Reward -2195\n",
      "Epoch 1843 Reward -3646\n",
      "Epoch 3137 Reward -4391\n",
      "Epoch 1662 Reward -2106\n",
      "Epoch 688 Reward -1402\n",
      "Epoch 2298 Reward -3993\n",
      "Epoch 2378 Reward -3569\n",
      "Epoch 5771 Reward -7871\n",
      "Epoch 800 Reward -813\n",
      "Epoch 7142 Reward -8567\n",
      "Epoch 1968 Reward -3699\n",
      "Epoch 861 Reward -801\n",
      "Epoch 228 Reward -79\n",
      "Epoch 4235 Reward -4886\n",
      "Epoch 3907 Reward -5809\n",
      "Epoch 1850 Reward -4976\n",
      "Epoch 825 Reward -1431\n",
      "Epoch 3421 Reward -5629\n",
      "Epoch 756 Reward -957\n",
      "Epoch 544 Reward -511\n",
      "Epoch 599 Reward -809\n",
      "Epoch 597 Reward -1473\n",
      "Epoch 798 Reward -954\n",
      "Epoch 1845 Reward -4161\n",
      "Epoch 5077 Reward -6034\n",
      "Epoch 6053 Reward -7433\n",
      "Epoch 6717 Reward -9339\n",
      "Epoch 2833 Reward -4627\n",
      "Epoch 2413 Reward -3019\n",
      "Epoch 364 Reward -547\n",
      "Epoch 6260 Reward -9818\n",
      "Epoch 4195 Reward -5431\n",
      "Epoch 3570 Reward -5022\n",
      "Epoch 3912 Reward -4851\n",
      "Epoch 2044 Reward -2497\n",
      "Epoch 380 Reward -437\n",
      "Epoch 5426 Reward -6887\n",
      "Epoch 2234 Reward -2669\n",
      "Epoch 11105 Reward -13403\n",
      "Epoch 1487 Reward -2012\n",
      "Epoch 2237 Reward -4337\n",
      "Epoch 1434 Reward -2607\n",
      "Epoch 837 Reward -1227\n",
      "Epoch 1218 Reward -1293\n",
      "Epoch 1789 Reward -2926\n",
      "Epoch 1036 Reward -1633\n",
      "Epoch 1669 Reward -2167\n",
      "Epoch 2452 Reward -3886\n",
      "Epoch 1077 Reward -1512\n",
      "Epoch 741 Reward -1113\n",
      "Epoch 1120 Reward -1438\n",
      "Epoch 4586 Reward -5588\n",
      "Epoch 1521 Reward -2001\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-6a7796fb5e96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msteps_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0;31m# 샘플 경험\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                 \u001b[0mo_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo_act\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo_next_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo_rew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_memories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0;31m# 상태\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-059abcc5dd96>\u001b[0m in \u001b[0;36msample_memories\u001b[0;34m(batch_size)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msample_memories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mperm_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mmem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mperm_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# tensorflow와 그 안의 모델을 실행\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    # 로그를 기록할 파일 열기\n",
    "    log_file = \"log.txt\"\n",
    "    \n",
    "    #에피소드\n",
    "    for i in range(num_episodes):\n",
    "        done = False\n",
    "        obs = env.reset()\n",
    "        epoch = 0\n",
    "        episodic_reward = 0\n",
    "        actions_counter = Counter()\n",
    "        episodic_loss = []\n",
    "        \n",
    "        #상태가 최종 상태가 아닌 동안\n",
    "        while not done:\n",
    "            # 전처리된 게임 화면 가져오기\n",
    "            obs = env.render('dqn')\n",
    "            \n",
    "            # 게임 화면을 피드하고 각 작업에 대한 Q 값을 가져오기\n",
    "            actions = mainQ_outputs.eval(feed_dict={X:[obs], in_training_mode: False})\n",
    "            \n",
    "            # 행동 가져오기\n",
    "            action = np.argmax(actions, axis =-1)\n",
    "            actions_counter[str(action)] += 1\n",
    "            \n",
    "            #엡실론 그리디 정책을 사용하여 행동 선택\n",
    "            action = epsilon_greedy(action, global_step)\n",
    "            \n",
    "            #행동을 수행하고 다음 상태인 next_obs로 이동하여 보상을 받는다.\n",
    "            next_obs, reward, done, _ = env.step(action)\n",
    "            \n",
    "            #이 전환을 재생 버퍼에 경험으로 저장\n",
    "            exp_buffer.append([obs, action, next_obs, reward, done])\n",
    "            \n",
    "            #특정 단계 후에 경험 버퍼의 샘플로 Q 네트워크를 훈련\n",
    "            if global_step % steps_train == 0 and global_step > start_steps:\n",
    "                # 샘플 경험\n",
    "                o_obs, o_act, o_next_obs, o_rew, o_done = sample_memories(batch_size)\n",
    "                \n",
    "                # 상태\n",
    "                o_obs = [x for x in o_obs]\n",
    "                \n",
    "                # 다음 상태\n",
    "                o_next_obs = [x for x in o_next_obs]\n",
    "                \n",
    "                # 다음 행동\n",
    "                next_act = mainQ_outputs.eval(feed_dict={X:o_next_obs, in_training_mode:False})\n",
    "                \n",
    "                # 보상\n",
    "                y_batch = o_rew + discount_factor * np.max(next_act, axis=-1) * (1-o_done)\n",
    "                \n",
    "                # 모든 요약을 병합하고 파일에 쓰기\n",
    "                mrg_summary = merge_summary.eval(feed_dict={X: o_obs, y:np.expand_dims(y_batch, axis=-1), X_action:o_act, in_training_mode:False})\n",
    "                file_writer.add_summary(mrg_summary, global_step)\n",
    "                \n",
    "                # 네트워크 훈련 및 loss 계산\n",
    "                train_loss, _ = sess.run([loss, training_op], feed_dict={X:o_obs, y:np.expand_dims(y_batch, axis=-1), X_action:o_act, in_training_mode: True})\n",
    "                episodic_loss.append(train_loss)\n",
    "                \n",
    "            # 일정 간격 후에 주요 Q 네트워크 가중치를 대상 Q 네트워크에 복사\n",
    "            if (global_step+1)%copy_steps == 0 and global_step > start_steps:\n",
    "                copy_target_to_main.run()\n",
    "                    \n",
    "            epoch += 1\n",
    "            global_step += 1\n",
    "            episodic_reward += reward\n",
    "            \n",
    "        # 파일에 로그 기록\n",
    "        with open(log_file, \"a\") as file:\n",
    "            file.write(\"Epoch: \" + str(epoch) + \", Reward: \" + str(episodic_reward))\n",
    "            file.write(\", Loss: \" + str(np.mean(episodic_loss)))\n",
    "            file.write(\", Actions: \" + str(actions_counter) + \"\\n\")\n",
    "        \n",
    "        # 정보 출력\n",
    "        print('Epoch', epoch, 'Reward', episodic_reward)\n",
    "        \n",
    "    # 저장 객체 생성\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    # 모델 저장\n",
    "    saver.save(sess, './model', global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fffa727e6d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o - - - - - - - - - - - - - - - - - o\n",
      "|   |       |           |           |\n",
      "|           | - |   |   | - | - |   |\n",
      "|   |     P         | 2 |           |\n",
      "| - |                       | - |   |\n",
      "|       |   |         1             |\n",
      "|   |         0             | - |   |\n",
      "|   |                       |       |\n",
      "|   | - | - | - |           | - |   |\n",
      "|                               | - |\n",
      "o - - - - - - - - - - - - - - - - - o\n",
      "\n",
      "Episode Reward=  -53\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-b0e96d4e997e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Episode Reward= \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fffa727e6d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tensorflow와 그 안의 모델을 실행\n",
    "with tf.Session() as sess:\n",
    "    # 저장 객체 생성\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    # 저장된 모델 로드\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('./')) \n",
    "\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "\n",
    "    #상태가 최종 상태가 아닌 동안\n",
    "    while not done:\n",
    "\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "        env.render()\n",
    "\n",
    "        # 전처리된 게임 화면 가져오기\n",
    "        obs = env.render('dqn')\n",
    "\n",
    "        # 게임 화면을 피드하고 각 작업에 대한 Q 값을 가져오기\n",
    "        actions = mainQ_outputs.eval(feed_dict={X:[obs], in_training_mode: False})\n",
    "\n",
    "        # 행동 가져오기\n",
    "        action = np.argmax(actions, axis =-1)\n",
    "\n",
    "        #행동을 수행하고 다음 상태인 next_obs로 이동하여 보상을 받는다.\n",
    "        next_obs, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        print(\"Episode Reward= \", total_reward)\n",
    "        time.sleep(0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
