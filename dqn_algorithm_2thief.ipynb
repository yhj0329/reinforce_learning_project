{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import flatten, conv2d, fully_connected\n",
    "from collections import deque, Counter\n",
    "import random\n",
    "from datetime import datetime\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from hide_and_seek_2thief import HideAndSeekEnv\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o - - - - - - - - - - - - - - - - - o\n",
      "| P |       |           |           |\n",
      "|           | - |   |   | - | - |   |\n",
      "|   |         3     |   |           |\n",
      "| - |                       | - |   |\n",
      "|       |   |                       |\n",
      "|   |                       | - |   |\n",
      "|   |                 0     |       |\n",
      "|   | - | - | - |           | - |   |\n",
      "|                               | - |\n",
      "o - - - - - - - - - - - - - - - - - o\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 환경 설정\n",
    "env = HideAndSeekEnv()\n",
    "n_outputs = env.action_space.n\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q 네트워크를 구축하기 위해 q_network라는 함수를 정의\n",
    "# Q 네트워크를 입력하고 해당 상태의 모든 작업에 대한 Q 값을 얻는다.\n",
    "# fully connected layer가 이어지는 동일한 패딩을 가진 3개의 convolutional layers로 Q 네트워크를 구축\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def q_network(X, name_scope):\n",
    "    #layers 초기화\n",
    "    initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "    \n",
    "    with tf.variable_scope(name_scope) as scope:\n",
    "        #convolutional layers 초기화\n",
    "        layer_1 = conv2d(X, num_outputs=32, kernel_size=(5,5), stride=4, padding=\"SAME\", weights_initializer=initializer)\n",
    "        tf.summary.histogram('layer_1',layer_1)\n",
    "        \n",
    "        layer_2 = conv2d(layer_1, num_outputs=64, kernel_size=(3,3), stride=2, padding=\"SAME\", weights_initializer=initializer)\n",
    "        tf.summary.histogram('layer_2',layer_2)\n",
    "        \n",
    "        layer_3 = conv2d(layer_2, num_outputs=64, kernel_size=(3,3), stride=1, padding=\"SAME\", weights_initializer=initializer)\n",
    "        tf.summary.histogram('layer_3',layer_3)\n",
    "        \n",
    "        # fully connected layer에 공급하기 전에 layer_3의 결과를 평탄화\n",
    "        flat = flatten(layer_3)\n",
    "        \n",
    "        fc = fully_connected(flat, num_outputs=128, weights_initializer=initializer)\n",
    "        tf.summary.histogram('fc',fc)\n",
    "        \n",
    "        output = fully_connected(fc, num_outputs=n_outputs, activation_fn=None, weights_initializer=initializer)\n",
    "        tf.summary.histogram('output',output)\n",
    "        \n",
    "        #vars는 가중치와 같은 네트워크 매개변수를 저장\n",
    "        vars = {v.name[len(scope.name):]: v for v in tf.get_collection(key=tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope.name)}\n",
    "        return vars, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 엡실론 그리디 정책을 수행하기 위해 epsilon_greedy라는 함수를 정의\n",
    "# 영원히 탐색하고 싶지 않기 때문에 엡실론의 가치가 시간이 지남에 따라 쇠퇴하는 쇠퇴 엡실론 탐욕 정책을 사용\n",
    "# 즉, 시간이 지남에 따라 우리 정책은 좋은 행동만 이용할 것입니다.\n",
    "eps_min=0.05\n",
    "eps_max=0.5\n",
    "eps_decay_steps = 500000\n",
    "\n",
    "def epsilon_greedy(action, step):\n",
    "    p = np.random.random(1).squeeze()\n",
    "    epsilon = max(eps_min, eps_max - (eps_max-eps_min) * step/eps_decay_steps)\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(n_outputs)\n",
    "    else:\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경험을 보유하는 50000의 경험 버퍼를 초기화\n",
    "# 에이전트의 모든 경험, 즉 (상태, 행동, 보상)을 경험 버퍼에 저장하고 네트워크 훈련을 위해 이 경험의 미니배치에서 샘플링\n",
    "buffer_len = 500000\n",
    "exp_buffer = deque(maxlen=buffer_len)\n",
    "\n",
    "# 메모리에서 경험을 샘플링하기 위해 sampled_memories라는 함수를 정의\n",
    "# 배치 크기는 메모리에서 샘플링된 경험의 수이다.\n",
    "def sample_memories(batch_size):\n",
    "    perm_batch = np.random.permutation(len(exp_buffer))[:batch_size]\n",
    "    mem = np.array(exp_buffer)[perm_batch]\n",
    "    return mem[:,0],mem[:,1],mem[:,2],mem[:,3],mem[:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 정의\n",
    "num_episodes = 1000\n",
    "batch_size = 48\n",
    "input_shape = (None, 11, 19, 1)\n",
    "learning_rate = 0.001\n",
    "X_shape = (None, 11, 19, 1)\n",
    "discount_factor = 0.97\n",
    "\n",
    "global_step = 0\n",
    "copy_steps = 100\n",
    "steps_train = 4\n",
    "start_steps = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = 'ch8_logs'\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# 입력에 대한 게임 상태를 정의\n",
    "X = tf.placeholder(tf.float32, shape=X_shape)\n",
    "\n",
    "# 교육을 토글하기 위해 in_training_model이라는 부울을 정의\n",
    "in_training_mode = tf.placeholder(tf.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 대상 Q 네트워크를 구축\n",
    "# 입력 X를 취하고 상태의 모든 작업에 대해 Q 값을 생성하는 Q 네트워크를 구축\n",
    "mainQ, mainQ_outputs = q_network(X, 'mainQ')\n",
    "\n",
    "# 목표 Q 네트워크를 구축\n",
    "targetQ, targetQ_outputs = q_network(X, 'targetQ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 행동 값에 대한 자리 표시자 정의\n",
    "X_action = tf.placeholder(tf.int32, shape=(None,))\n",
    "Q_action = tf.reduce_sum(targetQ_outputs * tf.one_hot(X_action, n_outputs), axis=-1,keep_dims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 Q 네트워크 매개변수를 대상 Q 네트워크에 복사\n",
    "copy_op = [tf.assign(main_name, targetQ[var_name]) for var_name, main_name in mainQ.items()]\n",
    "copy_target_to_main = tf.group(*copy_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent optimizer를 사용하여 손실 계산 및 최적화\n",
    "\n",
    "# 행동에 대한 자리 표시자를 정의\n",
    "y = tf.placeholder(tf.float32, shape=(None,1))\n",
    "\n",
    "# 실제 값과 예측 값의 차이인 손실을 계산\n",
    "loss = tf.reduce_mean(tf.square(y - Q_action))\n",
    "\n",
    "# loss을 최소화하기 위해 adam optimizer를 사용\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "loss_summary = tf.summary.scalar('Loss', loss)\n",
    "merge_summary = tf.summary.merge_all()\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/usr/local/lib/python3.5/dist-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 971 Reward -6462\n",
      "Epoch 829 Reward -5537\n",
      "Epoch 709 Reward -4472\n",
      "Epoch 606 Reward -3685\n",
      "Epoch 235 Reward -731\n",
      "Epoch 842 Reward -5676\n",
      "Epoch 341 Reward -1822\n",
      "Epoch 504 Reward -2539\n",
      "Epoch 3022 Reward -20321\n",
      "Epoch 739 Reward -2738\n",
      "Epoch 190 Reward -479\n",
      "Epoch 933 Reward -5182\n",
      "Epoch 1302 Reward -6091\n",
      "Epoch 1174 Reward -3668\n",
      "Epoch 1151 Reward -4122\n",
      "Epoch 261 Reward -460\n",
      "Epoch 109 Reward 286\n",
      "Epoch 413 Reward -954\n",
      "Epoch 609 Reward -2810\n",
      "Epoch 155 Reward -349\n",
      "Epoch 580 Reward -1184\n",
      "Epoch 605 Reward -2253\n",
      "Epoch 369 Reward -631\n",
      "Epoch 431 Reward -1026\n",
      "Epoch 283 Reward -338\n",
      "Epoch 418 Reward -1490\n",
      "Epoch 386 Reward -1062\n",
      "Epoch 284 Reward -501\n",
      "Epoch 1748 Reward -6294\n",
      "Epoch 408 Reward -1759\n",
      "Epoch 404 Reward -981\n",
      "Epoch 462 Reward -1624\n",
      "Epoch 172 Reward -308\n",
      "Epoch 64 Reward 376\n",
      "Epoch 215 Reward -396\n",
      "Epoch 386 Reward -1143\n",
      "Epoch 446 Reward -1734\n",
      "Epoch 338 Reward -1230\n",
      "Epoch 451 Reward -1136\n",
      "Epoch 455 Reward -1527\n",
      "Epoch 1198 Reward -3728\n",
      "Epoch 828 Reward -2890\n",
      "Epoch 833 Reward -2706\n",
      "Epoch 147 Reward -31\n",
      "Epoch 82 Reward 187\n",
      "Epoch 515 Reward -1155\n",
      "Epoch 900 Reward -3511\n",
      "Epoch 702 Reward -1603\n",
      "Epoch 82 Reward 376\n",
      "Epoch 1130 Reward -3489\n",
      "Epoch 75 Reward 203\n",
      "Epoch 199 Reward -101\n",
      "Epoch 885 Reward -2650\n",
      "Epoch 2628 Reward -8767\n",
      "Epoch 483 Reward -1636\n",
      "Epoch 394 Reward -665\n",
      "Epoch 118 Reward 7\n",
      "Epoch 545 Reward -1914\n",
      "Epoch 116 Reward 216\n",
      "Epoch 89 Reward 369\n",
      "Epoch 186 Reward -196\n",
      "Epoch 329 Reward -357\n",
      "Epoch 35 Reward 459\n",
      "Epoch 246 Reward -652\n",
      "Epoch 592 Reward -1475\n",
      "Epoch 741 Reward -2731\n",
      "Epoch 369 Reward -1027\n",
      "Epoch 97 Reward 190\n",
      "Epoch 217 Reward -515\n",
      "Epoch 94 Reward 202\n",
      "Epoch 376 Reward -1466\n",
      "Epoch 646 Reward -1970\n",
      "Epoch 120 Reward 230\n",
      "Epoch 397 Reward -1001\n",
      "Epoch 48 Reward 383\n",
      "Epoch 208 Reward -317\n",
      "Epoch 767 Reward -2676\n",
      "Epoch 738 Reward -2575\n",
      "Epoch 376 Reward -1151\n",
      "Epoch 363 Reward -778\n",
      "Epoch 955 Reward -2450\n",
      "Epoch 177 Reward -394\n",
      "Epoch 135 Reward -82\n",
      "Epoch 339 Reward -826\n",
      "Epoch 1385 Reward -4302\n",
      "Epoch 890 Reward -3348\n",
      "Epoch 252 Reward -856\n",
      "Epoch 928 Reward -2252\n",
      "Epoch 235 Reward -263\n",
      "Epoch 316 Reward -551\n",
      "Epoch 297 Reward -505\n",
      "Epoch 581 Reward -1572\n",
      "Epoch 200 Reward -138\n",
      "Epoch 739 Reward -1883\n",
      "Epoch 586 Reward -2135\n",
      "Epoch 21 Reward 518\n",
      "Epoch 391 Reward -725\n",
      "Epoch 40 Reward 391\n",
      "Epoch 278 Reward -396\n",
      "Epoch 366 Reward -871\n",
      "Epoch 131 Reward 156\n",
      "Epoch 152 Reward 234\n",
      "Epoch 876 Reward -2902\n",
      "Epoch 546 Reward -2329\n",
      "Epoch 277 Reward -746\n",
      "Epoch 345 Reward -670\n",
      "Epoch 66 Reward 302\n",
      "Epoch 35 Reward 468\n",
      "Epoch 111 Reward 104\n",
      "Epoch 73 Reward 340\n",
      "Epoch 225 Reward -604\n",
      "Epoch 390 Reward -1381\n",
      "Epoch 320 Reward -816\n",
      "Epoch 445 Reward -1103\n",
      "Epoch 363 Reward -1291\n",
      "Epoch 269 Reward -540\n",
      "Epoch 44 Reward 351\n",
      "Epoch 120 Reward 50\n",
      "Epoch 368 Reward -999\n",
      "Epoch 335 Reward -525\n",
      "Epoch 183 Reward -355\n",
      "Epoch 502 Reward -1475\n",
      "Epoch 203 Reward -393\n",
      "Epoch 427 Reward -1229\n",
      "Epoch 879 Reward -2905\n",
      "Epoch 98 Reward 90\n",
      "Epoch 811 Reward -2675\n",
      "Epoch 48 Reward 455\n",
      "Epoch 53 Reward 396\n",
      "Epoch 436 Reward -743\n",
      "Epoch 134 Reward 252\n",
      "Epoch 84 Reward 284\n",
      "Epoch 432 Reward -1198\n",
      "Epoch 511 Reward -1772\n",
      "Epoch 40 Reward 427\n",
      "Epoch 293 Reward -726\n",
      "Epoch 75 Reward 212\n",
      "Epoch 343 Reward -1118\n",
      "Epoch 353 Reward -1164\n",
      "Epoch 531 Reward -1495\n",
      "Epoch 583 Reward -1286\n",
      "Epoch 497 Reward -2082\n",
      "Epoch 59 Reward 318\n",
      "Epoch 686 Reward -1506\n",
      "Epoch 90 Reward 197\n",
      "Epoch 867 Reward -3901\n",
      "Epoch 70 Reward 388\n",
      "Epoch 348 Reward -970\n",
      "Epoch 499 Reward -1697\n",
      "Epoch 251 Reward -495\n",
      "Epoch 681 Reward -1312\n",
      "Epoch 182 Reward -462\n",
      "Epoch 224 Reward -162\n",
      "Epoch 115 Reward 190\n",
      "Epoch 470 Reward -1038\n",
      "Epoch 286 Reward -494\n",
      "Epoch 302 Reward -402\n",
      "Epoch 183 Reward -31\n",
      "Epoch 70 Reward 298\n",
      "Epoch 346 Reward -1013\n",
      "Epoch 284 Reward -870\n",
      "Epoch 528 Reward -1168\n",
      "Epoch 537 Reward -907\n",
      "Epoch 351 Reward -1234\n",
      "Epoch 544 Reward -2525\n",
      "Epoch 538 Reward -1817\n",
      "Epoch 85 Reward 355\n",
      "Epoch 510 Reward -1267\n",
      "Epoch 378 Reward -1126\n",
      "Epoch 359 Reward -612\n",
      "Epoch 99 Reward 260\n",
      "Epoch 324 Reward -622\n",
      "Epoch 265 Reward -572\n",
      "Epoch 406 Reward -884\n",
      "Epoch 154 Reward -362\n",
      "Epoch 243 Reward -253\n",
      "Epoch 86 Reward 201\n",
      "Epoch 250 Reward -71\n",
      "Epoch 668 Reward -1209\n",
      "Epoch 322 Reward -242\n",
      "Epoch 353 Reward -354\n",
      "Epoch 368 Reward -1575\n",
      "Epoch 47 Reward 483\n",
      "Epoch 117 Reward 35\n",
      "Epoch 231 Reward -268\n",
      "Epoch 254 Reward -345\n",
      "Epoch 716 Reward -2544\n",
      "Epoch 594 Reward -2782\n",
      "Epoch 70 Reward 298\n",
      "Epoch 324 Reward -622\n",
      "Epoch 494 Reward -1215\n",
      "Epoch 530 Reward -2250\n",
      "Epoch 212 Reward -573\n",
      "Epoch 203 Reward 39\n",
      "Epoch 84 Reward 212\n",
      "Epoch 299 Reward -282\n",
      "Epoch 179 Reward -369\n",
      "Epoch 54 Reward 413\n",
      "Epoch 313 Reward -791\n",
      "Epoch 660 Reward -1174\n",
      "Epoch 62 Reward 504\n",
      "Epoch 80 Reward 369\n",
      "Epoch 734 Reward -1806\n",
      "Epoch 49 Reward 364\n",
      "Epoch 173 Reward 213\n",
      "Epoch 483 Reward -1168\n",
      "Epoch 435 Reward -688\n",
      "Epoch 119 Reward 267\n",
      "Epoch 566 Reward -1377\n",
      "Epoch 1844 Reward -4329\n",
      "Epoch 419 Reward -1014\n",
      "Epoch 211 Reward -140\n",
      "Epoch 481 Reward -1580\n",
      "Epoch 226 Reward 142\n",
      "Epoch 349 Reward -980\n",
      "Epoch 430 Reward -1142\n",
      "Epoch 163 Reward 124\n",
      "Epoch 353 Reward -516\n",
      "Epoch 235 Reward -470\n",
      "Epoch 28 Reward 493\n",
      "Epoch 350 Reward -162\n",
      "Epoch 354 Reward -328\n",
      "Epoch 536 Reward -2400\n",
      "Epoch 121 Reward 112\n",
      "Epoch 111 Reward 230\n",
      "Epoch 386 Reward -1080\n",
      "Epoch 285 Reward -196\n",
      "Epoch 111 Reward 55\n",
      "Epoch 23 Reward 516\n",
      "Epoch 64 Reward 394\n",
      "Epoch 814 Reward -3056\n",
      "Epoch 942 Reward -4084\n",
      "Epoch 50 Reward 336\n",
      "Epoch 266 Reward -222\n",
      "Epoch 285 Reward -700\n",
      "Epoch 68 Reward 300\n",
      "Epoch 224 Reward -540\n",
      "Epoch 425 Reward -462\n",
      "Epoch 66 Reward 401\n",
      "Epoch 255 Reward -391\n",
      "Epoch 133 Reward 262\n",
      "Epoch 367 Reward -935\n",
      "Epoch 354 Reward -535\n",
      "Epoch 156 Reward -94\n",
      "Epoch 467 Reward -1782\n",
      "Epoch 218 Reward -66\n",
      "Epoch 152 Reward 144\n",
      "Epoch 761 Reward -1680\n",
      "Epoch 271 Reward -236\n",
      "Epoch 54 Reward 332\n",
      "Epoch 145 Reward -20\n",
      "Epoch 95 Reward 300\n",
      "Epoch 300 Reward -328\n",
      "Epoch 64 Reward 376\n",
      "Epoch 220 Reward -284\n",
      "Epoch 217 Reward -326\n",
      "Epoch 627 Reward -1132\n",
      "Epoch 197 Reward 90\n",
      "Epoch 288 Reward -622\n",
      "Epoch 680 Reward -1846\n",
      "Epoch 186 Reward -268\n",
      "Epoch 19 Reward 538\n",
      "Epoch 351 Reward -856\n",
      "Epoch 282 Reward -607\n",
      "Epoch 161 Reward -216\n",
      "Epoch 115 Reward 46\n",
      "Epoch 259 Reward -62\n",
      "Epoch 632 Reward -2307\n",
      "Epoch 217 Reward -281\n",
      "Epoch 345 Reward -571\n",
      "Epoch 208 Reward 115\n",
      "Epoch 135 Reward 89\n",
      "Epoch 163 Reward 25\n",
      "Epoch 578 Reward -1992\n",
      "Epoch 10 Reward 574\n",
      "Epoch 38 Reward 456\n",
      "Epoch 393 Reward -673\n",
      "Epoch 253 Reward -92\n",
      "Epoch 446 Reward -1788\n",
      "Epoch 17 Reward 558\n",
      "Epoch 15 Reward 524\n",
      "Epoch 79 Reward 415\n",
      "Epoch 700 Reward -2321\n",
      "Epoch 341 Reward -1116\n",
      "Epoch 260 Reward -513\n",
      "Epoch 427 Reward -1076\n",
      "Epoch 792 Reward -2584\n",
      "Epoch 338 Reward -1239\n",
      "Epoch 248 Reward -600\n",
      "Epoch 154 Reward -245\n",
      "Epoch 189 Reward 107\n",
      "Epoch 359 Reward -837\n",
      "Epoch 205 Reward 28\n",
      "Epoch 111 Reward 176\n",
      "Epoch 599 Reward -1896\n",
      "Epoch 303 Reward -799\n",
      "Epoch 296 Reward -1008\n",
      "Epoch 162 Reward 242\n",
      "Epoch 51 Reward 434\n",
      "Epoch 38 Reward 501\n",
      "Epoch 113 Reward 93\n",
      "Epoch 163 Reward 142\n",
      "Epoch 139 Reward 301\n",
      "Epoch 885 Reward -1246\n",
      "Epoch 538 Reward -1421\n",
      "Epoch 338 Reward -618\n",
      "Epoch 62 Reward 396\n",
      "Epoch 225 Reward -289\n",
      "Epoch 177 Reward -88\n",
      "Epoch 186 Reward -421\n",
      "Epoch 217 Reward -146\n",
      "Epoch 579 Reward -1246\n",
      "Epoch 405 Reward -883\n",
      "Epoch 71 Reward 360\n",
      "Epoch 33 Reward 488\n",
      "Epoch 159 Reward 128\n",
      "Epoch 286 Reward -557\n",
      "Epoch 96 Reward 299\n",
      "Epoch 229 Reward -572\n",
      "Epoch 126 Reward 179\n",
      "Epoch 215 Reward 36\n",
      "Epoch 127 Reward 223\n",
      "Epoch 232 Reward -386\n",
      "Epoch 385 Reward -800\n",
      "Epoch 403 Reward -818\n",
      "Epoch 239 Reward -303\n",
      "Epoch 249 Reward -169\n",
      "Epoch 222 Reward -367\n",
      "Epoch 252 Reward -298\n",
      "Epoch 675 Reward -2917\n",
      "Epoch 320 Reward -1122\n",
      "Epoch 336 Reward -652\n",
      "Epoch 80 Reward 234\n",
      "Epoch 47 Reward 321\n",
      "Epoch 172 Reward 61\n",
      "Epoch 37 Reward 367\n",
      "Epoch 127 Reward 115\n",
      "Epoch 98 Reward 54\n",
      "Epoch 119 Reward 330\n",
      "Epoch 249 Reward -403\n",
      "Epoch 26 Reward 450\n",
      "Epoch 51 Reward 43\n",
      "Epoch 907 Reward -3518\n",
      "Epoch 113 Reward 120\n",
      "Epoch 258 Reward -421\n",
      "Epoch 121 Reward 184\n",
      "Epoch 283 Reward -842\n",
      "Epoch 121 Reward 76\n",
      "Epoch 31 Reward 189\n",
      "Epoch 447 Reward -1015\n",
      "Epoch 455 Reward -816\n",
      "Epoch 78 Reward 344\n",
      "Epoch 161 Reward 9\n",
      "Epoch 450 Reward -1144\n",
      "Epoch 378 Reward -1045\n",
      "Epoch 280 Reward -632\n",
      "Epoch 86 Reward 228\n",
      "Epoch 169 Reward -71\n",
      "Epoch 651 Reward -1732\n",
      "Epoch 78 Reward 227\n",
      "Epoch 92 Reward 186\n",
      "Epoch 92 Reward 123\n",
      "Epoch 179 Reward -72\n",
      "Epoch 192 Reward -292\n",
      "Epoch 301 Reward -653\n",
      "Epoch 184 Reward -104\n",
      "Epoch 111 Reward 185\n",
      "Epoch 481 Reward -1337\n",
      "Epoch 145 Reward -83\n",
      "Epoch 183 Reward 86\n",
      "Epoch 54 Reward 359\n",
      "Epoch 508 Reward -1859\n",
      "Epoch 119 Reward -21\n",
      "Epoch 238 Reward -653\n",
      "Epoch 409 Reward -1463\n",
      "Epoch 208 Reward -213\n",
      "Epoch 239 Reward 30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74 Reward 213\n",
      "Epoch 50 Reward 291\n",
      "Epoch 40 Reward 463\n",
      "Epoch 271 Reward -353\n",
      "Epoch 106 Reward 343\n",
      "Epoch 305 Reward -414\n",
      "Epoch 179 Reward 9\n",
      "Epoch 147 Reward 149\n",
      "Epoch 184 Reward 139\n",
      "Epoch 559 Reward -2054\n",
      "Epoch 61 Reward 415\n",
      "Epoch 43 Reward 433\n",
      "Epoch 101 Reward 132\n",
      "Epoch 292 Reward -626\n",
      "Epoch 969 Reward -3382\n",
      "Epoch 70 Reward 316\n",
      "Epoch 55 Reward 403\n",
      "Epoch 517 Reward -1904\n",
      "Epoch 381 Reward -1030\n",
      "Epoch 60 Reward 245\n",
      "Epoch 255 Reward -1075\n",
      "Epoch 376 Reward -1205\n",
      "Epoch 190 Reward -2\n",
      "Epoch 264 Reward -400\n",
      "Epoch 124 Reward -98\n",
      "Epoch 137 Reward 222\n",
      "Epoch 175 Reward -185\n",
      "Epoch 236 Reward -534\n",
      "Epoch 243 Reward -118\n",
      "Epoch 148 Reward -5\n",
      "Epoch 47 Reward 366\n",
      "Epoch 94 Reward 328\n",
      "Epoch 33 Reward 452\n",
      "Epoch 118 Reward -29\n",
      "Epoch 230 Reward -222\n",
      "Epoch 87 Reward 218\n",
      "Epoch 84 Reward 203\n",
      "Epoch 77 Reward 255\n",
      "Epoch 67 Reward 265\n",
      "Epoch 281 Reward -606\n",
      "Epoch 184 Reward -14\n",
      "Epoch 590 Reward -2337\n",
      "Epoch 660 Reward -904\n",
      "Epoch 276 Reward -817\n",
      "Epoch 52 Reward 397\n",
      "Epoch 312 Reward -682\n",
      "Epoch 74 Reward 231\n",
      "Epoch 287 Reward -684\n",
      "Epoch 189 Reward -199\n",
      "Epoch 16 Reward 541\n",
      "Epoch 127 Reward -101\n",
      "Epoch 417 Reward -1264\n",
      "Epoch 88 Reward 145\n",
      "Epoch 292 Reward -1004\n",
      "Epoch 176 Reward -501\n",
      "Epoch 195 Reward -115\n",
      "Epoch 72 Reward 350\n",
      "Epoch 416 Reward -948\n",
      "Epoch 623 Reward -2424\n",
      "Epoch 166 Reward 31\n",
      "Epoch 104 Reward 183\n",
      "Epoch 65 Reward 303\n",
      "Epoch 603 Reward -2062\n",
      "Epoch 175 Reward 247\n",
      "Epoch 527 Reward -1869\n",
      "Epoch 233 Reward -234\n",
      "Epoch 312 Reward -466\n",
      "Epoch 593 Reward -2709\n",
      "Epoch 25 Reward 532\n",
      "Epoch 496 Reward -623\n",
      "Epoch 420 Reward -1852\n",
      "Epoch 356 Reward -870\n",
      "Epoch 787 Reward -3578\n",
      "Epoch 212 Reward -123\n",
      "Epoch 820 Reward -1757\n",
      "Epoch 187 Reward -386\n",
      "Epoch 143 Reward 162\n",
      "Epoch 500 Reward -1140\n",
      "Epoch 155 Reward 24\n",
      "Epoch 481 Reward -896\n",
      "Epoch 93 Reward 248\n",
      "Epoch 303 Reward -205\n",
      "Epoch 206 Reward -216\n",
      "Epoch 202 Reward 166\n",
      "Epoch 45 Reward 386\n",
      "Epoch 234 Reward -478\n",
      "Epoch 289 Reward -488\n",
      "Epoch 151 Reward -17\n",
      "Epoch 161 Reward -234\n",
      "Epoch 156 Reward 203\n",
      "Epoch 137 Reward 231\n",
      "Epoch 303 Reward -322\n",
      "Epoch 172 Reward 196\n",
      "Epoch 222 Reward -682\n",
      "Epoch 131 Reward -204\n",
      "Epoch 139 Reward -95\n",
      "Epoch 343 Reward -1505\n",
      "Epoch 307 Reward -515\n",
      "Epoch 68 Reward 363\n",
      "Epoch 321 Reward -736\n",
      "Epoch 137 Reward -39\n",
      "Epoch 265 Reward -176\n",
      "Epoch 205 Reward -116\n",
      "Epoch 76 Reward 382\n",
      "Epoch 319 Reward -824\n",
      "Epoch 114 Reward 83\n",
      "Epoch 145 Reward 124\n",
      "Epoch 199 Reward -2\n",
      "Epoch 271 Reward -605\n",
      "Epoch 119 Reward 186\n",
      "Epoch 107 Reward -99\n",
      "Epoch 96 Reward 425\n",
      "Epoch 256 Reward 112\n",
      "Epoch 51 Reward 308\n",
      "Epoch 130 Reward -14\n",
      "Epoch 195 Reward -124\n",
      "Epoch 108 Reward 143\n",
      "Epoch 227 Reward -300\n",
      "Epoch 394 Reward -1439\n",
      "Epoch 30 Reward 244\n",
      "Epoch 225 Reward -10\n",
      "Epoch 59 Reward 417\n",
      "Epoch 150 Reward 218\n",
      "Epoch 225 Reward -235\n",
      "Epoch 186 Reward -232\n",
      "Epoch 223 Reward -260\n",
      "Epoch 81 Reward 377\n",
      "Epoch 79 Reward 388\n",
      "Epoch 54 Reward 359\n",
      "Epoch 341 Reward -1062\n",
      "Epoch 54 Reward 350\n",
      "Epoch 136 Reward -20\n",
      "Epoch 171 Reward 242\n",
      "Epoch 203 Reward -204\n",
      "Epoch 310 Reward -419\n",
      "Epoch 78 Reward 389\n",
      "Epoch 106 Reward 154\n",
      "Epoch 250 Reward -593\n",
      "Epoch 150 Reward 56\n",
      "Epoch 192 Reward -391\n",
      "Epoch 155 Reward -183\n",
      "Epoch 117 Reward 161\n",
      "Epoch 273 Reward -292\n",
      "Epoch 303 Reward -673\n",
      "Epoch 63 Reward 314\n",
      "Epoch 51 Reward 362\n",
      "Epoch 156 Reward -85\n",
      "Epoch 69 Reward 407\n",
      "Epoch 135 Reward -1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-bcd06a0336a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msteps_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0;31m# 샘플 경험\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                 \u001b[0mo_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo_act\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo_next_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo_rew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_memories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0;31m# 상태\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-059abcc5dd96>\u001b[0m in \u001b[0;36msample_memories\u001b[0;34m(batch_size)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msample_memories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mperm_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mmem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mperm_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# tensorflow와 그 안의 모델을 실행\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    # 로그를 기록할 파일 열기\n",
    "    log_file = \"log.txt\"\n",
    "    \n",
    "    #에피소드\n",
    "    for i in range(num_episodes):\n",
    "        done = False\n",
    "        obs = env.reset()\n",
    "        epoch = 0\n",
    "        episodic_reward = 0\n",
    "        actions_counter = Counter()\n",
    "        episodic_loss = []\n",
    "        \n",
    "        #상태가 최종 상태가 아닌 동안\n",
    "        while not done:\n",
    "            # 전처리된 게임 화면 가져오기\n",
    "            obs = env.render('dqn')\n",
    "            \n",
    "            # 게임 화면을 피드하고 각 작업에 대한 Q 값을 가져오기\n",
    "            actions = mainQ_outputs.eval(feed_dict={X:[obs], in_training_mode: False})\n",
    "            \n",
    "            # 행동 가져오기\n",
    "            action = np.argmax(actions, axis =-1)\n",
    "            actions_counter[str(action)] += 1\n",
    "            \n",
    "            #엡실론 그리디 정책을 사용하여 행동 선택\n",
    "            action = epsilon_greedy(action, global_step)\n",
    "            \n",
    "            #행동을 수행하고 다음 상태인 next_obs로 이동하여 보상을 받는다.\n",
    "            next_obs, reward, done, _ = env.step(action)\n",
    "            \n",
    "            #이 전환을 재생 버퍼에 경험으로 저장\n",
    "            exp_buffer.append([obs, action, env.render('dqn'), reward, done])\n",
    "            \n",
    "            #특정 단계 후에 경험 버퍼의 샘플로 Q 네트워크를 훈련\n",
    "            if global_step % steps_train == 0 and global_step > start_steps:\n",
    "                # 샘플 경험\n",
    "                o_obs, o_act, o_next_obs, o_rew, o_done = sample_memories(batch_size)\n",
    "                \n",
    "                # 상태\n",
    "                o_obs = [x for x in o_obs]\n",
    "                \n",
    "                # 다음 상태\n",
    "                o_next_obs = [x for x in o_next_obs]\n",
    "                \n",
    "                # 다음 행동\n",
    "                next_act = mainQ_outputs.eval(feed_dict={X:o_next_obs, in_training_mode:False})\n",
    "                \n",
    "                # 보상\n",
    "                y_batch = o_rew + discount_factor * np.max(next_act, axis=-1) * (1-o_done)\n",
    "                \n",
    "                # 모든 요약을 병합하고 파일에 쓰기\n",
    "                mrg_summary = merge_summary.eval(feed_dict={X: o_obs, y:np.expand_dims(y_batch, axis=-1), X_action:o_act, in_training_mode:False})\n",
    "                file_writer.add_summary(mrg_summary, global_step)\n",
    "                \n",
    "                # 네트워크 훈련 및 loss 계산\n",
    "                train_loss, _ = sess.run([loss, training_op], feed_dict={X:o_obs, y:np.expand_dims(y_batch, axis=-1), X_action:o_act, in_training_mode: True})\n",
    "                episodic_loss.append(train_loss)\n",
    "                \n",
    "            # 일정 간격 후에 주요 Q 네트워크 가중치를 대상 Q 네트워크에 복사\n",
    "            if (global_step+1)%copy_steps == 0 and global_step > start_steps:\n",
    "                copy_target_to_main.run()\n",
    "                    \n",
    "            epoch += 1\n",
    "            global_step += 1\n",
    "            episodic_reward += reward\n",
    "            \n",
    "        # 파일에 로그 기록\n",
    "        with open(log_file, \"a\") as file:\n",
    "            file.write(\"Epoch: \" + str(epoch) + \", Reward: \" + str(episodic_reward))\n",
    "            file.write(\", Loss: \" + str(np.mean(episodic_loss)))\n",
    "            file.write(\", Actions: \" + str(actions_counter) + \"\\n\")\n",
    "        \n",
    "        # 정보 출력\n",
    "        print('Epoch', epoch, 'Reward', episodic_reward)\n",
    "        \n",
    "    # 저장 객체 생성\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    # 모델 저장\n",
    "    saver.save(sess, './model', global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fffa7285be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o - - - - - - - - - - - - - - - - - o\n",
      "| P |       |           |           |\n",
      "|           | - |   |   | - | - |   |\n",
      "|   |     2         |   |           |\n",
      "| - |                       | - |   |\n",
      "|       |   |                       |\n",
      "|   |                       | - |   |\n",
      "|   |                       |       |\n",
      "|   | - | - | - |           | - |   |\n",
      "|                         0     | - |\n",
      "o - - - - - - - - - - - - - - - - - o\n",
      "\n",
      "Episode Reward=  -380\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-b0e96d4e997e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Episode Reward= \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fffa7285be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tensorflow와 그 안의 모델을 실행\n",
    "with tf.Session() as sess:\n",
    "    # 저장 객체 생성\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    # 저장된 모델 로드\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('./')) \n",
    "\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "\n",
    "    #상태가 최종 상태가 아닌 동안\n",
    "    while not done:\n",
    "\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "        env.render()\n",
    "\n",
    "        # 전처리된 게임 화면 가져오기\n",
    "        obs = env.render('dqn')\n",
    "\n",
    "        # 게임 화면을 피드하고 각 작업에 대한 Q 값을 가져오기\n",
    "        actions = mainQ_outputs.eval(feed_dict={X:[obs], in_training_mode: False})\n",
    "\n",
    "        # 행동 가져오기\n",
    "        action = np.argmax(actions, axis =-1)\n",
    "\n",
    "        #행동을 수행하고 다음 상태인 next_obs로 이동하여 보상을 받는다.\n",
    "        next_obs, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        print(\"Episode Reward= \", total_reward)\n",
    "        time.sleep(0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
