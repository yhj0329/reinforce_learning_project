{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import flatten, conv2d, fully_connected\n",
    "from collections import deque, Counter\n",
    "import random\n",
    "from datetime import datetime\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from hide_and_seek_1thief import HideAndSeekEnv\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o - - - - - - - - - - - - - - - - - o\n",
      "| P |       |           |           |\n",
      "|           | - |   |   | - | - |   |\n",
      "|   |               |   |           |\n",
      "| - |                       | - |   |\n",
      "|       |   |                       |\n",
      "|   |                       | - |   |\n",
      "|   |                       |       |\n",
      "|   | - | - | - |           | - |   |\n",
      "|                         2     | - |\n",
      "o - - - - - - - - - - - - - - - - - o\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 환경 설정\n",
    "env = HideAndSeekEnv()\n",
    "n_outputs = env.action_space.n\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q 네트워크를 구축하기 위해 q_network라는 함수를 정의\n",
    "# Q 네트워크를 입력하고 해당 상태의 모든 작업에 대한 Q 값을 얻는다.\n",
    "# fully connected layer가 이어지는 동일한 패딩을 가진 3개의 convolutional layers로 Q 네트워크를 구축\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def q_network(X, name_scope):\n",
    "    #layers 초기화\n",
    "    initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "    \n",
    "    with tf.variable_scope(name_scope) as scope:\n",
    "        #convolutional layers 초기화\n",
    "        layer_1 = conv2d(X, num_outputs=32, kernel_size=(5,5), stride=4, padding=\"SAME\", weights_initializer=initializer)\n",
    "        tf.summary.histogram('layer_1',layer_1)\n",
    "        \n",
    "        layer_2 = conv2d(layer_1, num_outputs=64, kernel_size=(3,3), stride=2, padding=\"SAME\", weights_initializer=initializer)\n",
    "        tf.summary.histogram('layer_2',layer_2)\n",
    "        \n",
    "        layer_3 = conv2d(layer_2, num_outputs=64, kernel_size=(3,3), stride=1, padding=\"SAME\", weights_initializer=initializer)\n",
    "        tf.summary.histogram('layer_3',layer_3)\n",
    "        \n",
    "        # fully connected layer에 공급하기 전에 layer_3의 결과를 평탄화\n",
    "        flat = flatten(layer_3)\n",
    "        \n",
    "        fc = fully_connected(flat, num_outputs=128, weights_initializer=initializer)\n",
    "        tf.summary.histogram('fc',fc)\n",
    "        \n",
    "        output = fully_connected(fc, num_outputs=n_outputs, activation_fn=None, weights_initializer=initializer)\n",
    "        tf.summary.histogram('output',output)\n",
    "        \n",
    "        #vars는 가중치와 같은 네트워크 매개변수를 저장\n",
    "        vars = {v.name[len(scope.name):]: v for v in tf.get_collection(key=tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope.name)}\n",
    "        return vars, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 엡실론 그리디 정책을 수행하기 위해 epsilon_greedy라는 함수를 정의\n",
    "# 영원히 탐색하고 싶지 않기 때문에 엡실론의 가치가 시간이 지남에 따라 쇠퇴하는 쇠퇴 엡실론 탐욕 정책을 사용\n",
    "# 즉, 시간이 지남에 따라 우리 정책은 좋은 행동만 이용할 것입니다.\n",
    "eps_min=0.05\n",
    "eps_max=0.5\n",
    "eps_decay_steps = 500000\n",
    "\n",
    "def epsilon_greedy(action, step):\n",
    "    p = np.random.random(1).squeeze()\n",
    "    epsilon = max(eps_min, eps_max - (eps_max-eps_min) * step/eps_decay_steps)\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(n_outputs)\n",
    "    else:\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경험을 보유하는 50000의 경험 버퍼를 초기화\n",
    "# 에이전트의 모든 경험, 즉 (상태, 행동, 보상)을 경험 버퍼에 저장하고 네트워크 훈련을 위해 이 경험의 미니배치에서 샘플링\n",
    "buffer_len = 26244\n",
    "exp_buffer = deque(maxlen=buffer_len)\n",
    "\n",
    "# 메모리에서 경험을 샘플링하기 위해 sampled_memories라는 함수를 정의\n",
    "# 배치 크기는 메모리에서 샘플링된 경험의 수이다.\n",
    "def sample_memories(batch_size):\n",
    "    perm_batch = np.random.permutation(len(exp_buffer))[:batch_size]\n",
    "    mem = np.array(exp_buffer)[perm_batch]\n",
    "    return mem[:,0],mem[:,1],mem[:,2],mem[:,3],mem[:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 정의\n",
    "num_episodes = 2000\n",
    "batch_size = 48\n",
    "input_shape = (None, 11, 19, 1)\n",
    "learning_rate = 0.001\n",
    "X_shape = (None, 11, 19, 1)\n",
    "discount_factor = 0.97\n",
    "\n",
    "global_step = 0\n",
    "copy_steps = 100\n",
    "steps_train = 4\n",
    "start_steps = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = 'ch8_logs'\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# 입력에 대한 게임 상태를 정의\n",
    "X = tf.placeholder(tf.float32, shape=X_shape)\n",
    "\n",
    "# 교육을 토글하기 위해 in_training_model이라는 부울을 정의\n",
    "in_training_mode = tf.placeholder(tf.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 대상 Q 네트워크를 구축\n",
    "# 입력 X를 취하고 상태의 모든 작업에 대해 Q 값을 생성하는 Q 네트워크를 구축\n",
    "mainQ, mainQ_outputs = q_network(X, 'mainQ')\n",
    "\n",
    "# 목표 Q 네트워크를 구축\n",
    "targetQ, targetQ_outputs = q_network(X, 'targetQ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 행동 값에 대한 자리 표시자 정의\n",
    "X_action = tf.placeholder(tf.int32, shape=(None,))\n",
    "Q_action = tf.reduce_sum(targetQ_outputs * tf.one_hot(X_action, n_outputs), axis=-1,keep_dims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 Q 네트워크 매개변수를 대상 Q 네트워크에 복사\n",
    "copy_op = [tf.assign(main_name, targetQ[var_name]) for var_name, main_name in mainQ.items()]\n",
    "copy_target_to_main = tf.group(*copy_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent optimizer를 사용하여 손실 계산 및 최적화\n",
    "\n",
    "# 행동에 대한 자리 표시자를 정의\n",
    "y = tf.placeholder(tf.float32, shape=(None,1))\n",
    "\n",
    "# 실제 값과 예측 값의 차이인 손실을 계산\n",
    "loss = tf.reduce_mean(tf.square(y - Q_action))\n",
    "\n",
    "# loss을 최소화하기 위해 adam optimizer를 사용\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "loss_summary = tf.summary.scalar('Loss', loss)\n",
    "merge_summary = tf.summary.merge_all()\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/usr/local/lib/python3.5/dist-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 642 Reward -4519\n",
      "Epoch 1296 Reward -9754\n",
      "Epoch 70 Reward -320\n",
      "Epoch 599 Reward -3756\n",
      "Epoch 320 Reward -1929\n",
      "Epoch 2087 Reward -11319\n",
      "Epoch 28 Reward -89\n",
      "Epoch 354 Reward -1549\n",
      "Epoch 1140 Reward -5575\n",
      "Epoch 108 Reward -394\n",
      "Epoch 349 Reward -1400\n",
      "Epoch 1222 Reward -4406\n",
      "Epoch 4287 Reward -14473\n",
      "Epoch 1481 Reward -4710\n",
      "Epoch 409 Reward -1370\n",
      "Epoch 73 Reward -134\n",
      "Epoch 39 Reward -46\n",
      "Epoch 1355 Reward -4647\n",
      "Epoch 1518 Reward -4450\n",
      "Epoch 374 Reward -1155\n",
      "Epoch 493 Reward -1463\n",
      "Epoch 3289 Reward -10514\n",
      "Epoch 502 Reward -1274\n",
      "Epoch 975 Reward -2746\n",
      "Epoch 9 Reward 83\n",
      "Epoch 448 Reward -1553\n",
      "Epoch 3006 Reward -8431\n",
      "Epoch 875 Reward -2385\n",
      "Epoch 12 Reward 62\n",
      "Epoch 35 Reward 30\n",
      "Epoch 320 Reward -894\n",
      "Epoch 492 Reward -1543\n",
      "Epoch 27 Reward -7\n",
      "Epoch 298 Reward -791\n",
      "Epoch 706 Reward -1865\n",
      "Epoch 90 Reward -205\n",
      "Epoch 596 Reward -1647\n",
      "Epoch 63 Reward -34\n",
      "Epoch 1287 Reward -4030\n",
      "Epoch 360 Reward -1042\n",
      "Epoch 244 Reward -818\n",
      "Epoch 193 Reward -524\n",
      "Epoch 46 Reward 1\n",
      "Epoch 320 Reward -1155\n",
      "Epoch 224 Reward -546\n",
      "Epoch 103 Reward -137\n",
      "Epoch 208 Reward -422\n",
      "Epoch 223 Reward -923\n",
      "Epoch 341 Reward -1194\n",
      "Epoch 156 Reward -307\n",
      "Epoch 686 Reward -1404\n",
      "Epoch 170 Reward -411\n",
      "Epoch 1110 Reward -2629\n",
      "Epoch 428 Reward -948\n",
      "Epoch 14 Reward 87\n",
      "Epoch 101 Reward -252\n",
      "Epoch 590 Reward -1416\n",
      "Epoch 172 Reward -278\n",
      "Epoch 10 Reward 64\n",
      "Epoch 1048 Reward -2729\n",
      "Epoch 146 Reward -351\n",
      "Epoch 294 Reward -760\n",
      "Epoch 121 Reward -218\n",
      "Epoch 88 Reward -239\n",
      "Epoch 93 Reward -136\n",
      "Epoch 99 Reward -124\n",
      "Epoch 354 Reward -694\n",
      "Epoch 559 Reward -1493\n",
      "Epoch 317 Reward -639\n",
      "Epoch 709 Reward -1688\n",
      "Epoch 2 Reward 90\n",
      "Epoch 309 Reward -955\n",
      "Epoch 1444 Reward -3368\n",
      "Epoch 324 Reward -844\n",
      "Epoch 270 Reward -583\n",
      "Epoch 185 Reward -399\n",
      "Epoch 48 Reward 8\n",
      "Epoch 3 Reward 89\n",
      "Epoch 28 Reward 1\n",
      "Epoch 169 Reward -230\n",
      "Epoch 237 Reward -496\n",
      "Epoch 289 Reward -548\n",
      "Epoch 79 Reward -122\n",
      "Epoch 240 Reward -508\n",
      "Epoch 186 Reward -382\n",
      "Epoch 212 Reward -642\n",
      "Epoch 77 Reward -102\n",
      "Epoch 252 Reward -466\n",
      "Epoch 172 Reward -530\n",
      "Epoch 243 Reward -466\n",
      "Epoch 286 Reward -545\n",
      "Epoch 258 Reward -544\n",
      "Epoch 99 Reward -115\n",
      "Epoch 679 Reward -1208\n",
      "Epoch 269 Reward -744\n",
      "Epoch 29 Reward 54\n",
      "Epoch 510 Reward -1084\n",
      "Epoch 37 Reward -44\n",
      "Epoch 172 Reward -476\n",
      "Epoch 30 Reward 8\n",
      "Epoch 28 Reward 64\n",
      "Epoch 101 Reward -72\n",
      "Epoch 124 Reward -266\n",
      "Epoch 99 Reward -106\n",
      "Epoch 295 Reward -842\n",
      "Epoch 116 Reward -222\n",
      "Epoch 28 Reward 28\n",
      "Epoch 270 Reward -610\n",
      "Epoch 740 Reward -1845\n",
      "Epoch 491 Reward -1515\n",
      "Epoch 103 Reward -245\n",
      "Epoch 595 Reward -2186\n",
      "Epoch 132 Reward -643\n",
      "Epoch 594 Reward -2014\n",
      "Epoch 245 Reward -432\n",
      "Epoch 159 Reward -364\n",
      "Epoch 62 Reward -24\n",
      "Epoch 118 Reward -152\n",
      "Epoch 88 Reward -176\n",
      "Epoch 572 Reward -1533\n",
      "Epoch 32 Reward -48\n",
      "Epoch 253 Reward -890\n",
      "Epoch 117 Reward -196\n",
      "Epoch 127 Reward -224\n",
      "Epoch 116 Reward -294\n",
      "Epoch 551 Reward -1962\n",
      "Epoch 86 Reward -84\n",
      "Epoch 81 Reward -88\n",
      "Epoch 189 Reward -394\n",
      "Epoch 150 Reward -382\n",
      "Epoch 138 Reward -334\n",
      "Epoch 97 Reward -113\n",
      "Epoch 14 Reward 60\n",
      "Epoch 323 Reward -600\n",
      "Epoch 71 Reward -60\n",
      "Epoch 307 Reward -692\n",
      "Epoch 225 Reward -412\n",
      "Epoch 237 Reward -397\n",
      "Epoch 132 Reward -274\n",
      "Epoch 44 Reward -42\n",
      "Epoch 175 Reward -308\n",
      "Epoch 431 Reward -780\n",
      "Epoch 291 Reward -469\n",
      "Epoch 607 Reward -1109\n",
      "Epoch 148 Reward -272\n",
      "Epoch 32 Reward 42\n",
      "Epoch 231 Reward -436\n",
      "Epoch 110 Reward -162\n",
      "Epoch 256 Reward -515\n",
      "Epoch 87 Reward -121\n",
      "Epoch 246 Reward -487\n",
      "Epoch 264 Reward -523\n",
      "Epoch 697 Reward -1739\n",
      "Epoch 57 Reward -46\n",
      "Epoch 13 Reward 70\n",
      "Epoch 279 Reward -826\n",
      "Epoch 541 Reward -1196\n",
      "Epoch 71 Reward -78\n",
      "Epoch 331 Reward -662\n",
      "Epoch 122 Reward -201\n",
      "Epoch 434 Reward -1017\n",
      "Epoch 358 Reward -770\n",
      "Epoch 27 Reward 74\n",
      "Epoch 121 Reward -308\n",
      "Epoch 785 Reward -2097\n",
      "Epoch 42 Reward 5\n",
      "Epoch 51 Reward -13\n",
      "Epoch 139 Reward -344\n",
      "Epoch 88 Reward -77\n",
      "Epoch 321 Reward -715\n",
      "Epoch 125 Reward -159\n",
      "Epoch 19 Reward 73\n",
      "Epoch 193 Reward -317\n",
      "Epoch 44 Reward -15\n",
      "Epoch 18 Reward 65\n",
      "Epoch 20 Reward 45\n",
      "Epoch 22 Reward 52\n",
      "Epoch 244 Reward -683\n",
      "Epoch 411 Reward -1975\n",
      "Epoch 546 Reward -1453\n",
      "Epoch 1130 Reward -2667\n",
      "Epoch 48 Reward -1\n",
      "Epoch 364 Reward -749\n",
      "Epoch 51 Reward -22\n",
      "Epoch 636 Reward -1309\n",
      "Epoch 236 Reward -459\n",
      "Epoch 88 Reward -122\n",
      "Epoch 11 Reward 45\n",
      "Epoch 11 Reward 72\n",
      "Epoch 95 Reward -138\n",
      "Epoch 211 Reward -326\n",
      "Epoch 589 Reward -1280\n",
      "Epoch 35 Reward 21\n",
      "Epoch 110 Reward -117\n",
      "Epoch 209 Reward -540\n",
      "Epoch 118 Reward -161\n",
      "Epoch 13 Reward 70\n",
      "Epoch 78 Reward -49\n",
      "Epoch 65 Reward -72\n",
      "Epoch 153 Reward -214\n",
      "Epoch 108 Reward -151\n",
      "Epoch 263 Reward -495\n",
      "Epoch 1 Reward 100\n",
      "Epoch 52 Reward -23\n",
      "Epoch 527 Reward -957\n",
      "Epoch 131 Reward -165\n",
      "Epoch 190 Reward -323\n",
      "Epoch 157 Reward -218\n",
      "Epoch 62 Reward -114\n",
      "Epoch 380 Reward -2637\n",
      "Epoch 504 Reward -2617\n",
      "Epoch 636 Reward -1579\n",
      "Epoch 401 Reward -975\n",
      "Epoch 352 Reward -764\n",
      "Epoch 567 Reward -1573\n",
      "Epoch 159 Reward -328\n",
      "Epoch 179 Reward -276\n",
      "Epoch 110 Reward -144\n",
      "Epoch 534 Reward -1000\n",
      "Epoch 43 Reward 22\n",
      "Epoch 55 Reward -8\n",
      "Epoch 398 Reward -774\n",
      "Epoch 14 Reward 60\n",
      "Epoch 31 Reward 34\n",
      "Epoch 48 Reward -28\n",
      "Epoch 83 Reward -180\n",
      "Epoch 344 Reward -792\n",
      "Epoch 60 Reward -58\n",
      "Epoch 211 Reward -389\n",
      "Epoch 85 Reward -101\n",
      "Epoch 77 Reward -66\n",
      "Epoch 139 Reward -218\n",
      "Epoch 890 Reward -2670\n",
      "Epoch 312 Reward -697\n",
      "Epoch 35 Reward -51\n",
      "Epoch 264 Reward -478\n",
      "Epoch 160 Reward -302\n",
      "Epoch 285 Reward -625\n",
      "Epoch 2 Reward 99\n",
      "Epoch 100 Reward -449\n",
      "Epoch 25 Reward -5\n",
      "Epoch 528 Reward -1777\n",
      "Epoch 148 Reward -335\n",
      "Epoch 311 Reward -768\n",
      "Epoch 19 Reward 55\n",
      "Epoch 236 Reward -369\n",
      "Epoch 251 Reward -555\n",
      "Epoch 198 Reward -367\n",
      "Epoch 145 Reward -314\n",
      "Epoch 117 Reward -160\n",
      "Epoch 95 Reward -129\n",
      "Epoch 237 Reward -451\n",
      "Epoch 159 Reward -229\n",
      "Epoch 215 Reward -501\n",
      "Epoch 12 Reward 71\n",
      "Epoch 514 Reward -1241\n",
      "Epoch 3 Reward 80\n",
      "Epoch 44 Reward -24\n",
      "Epoch 131 Reward -273\n",
      "Epoch 16 Reward 76\n",
      "Epoch 145 Reward -341\n",
      "Epoch 112 Reward -200\n",
      "Epoch 408 Reward -847\n",
      "Epoch 125 Reward -186\n",
      "Epoch 461 Reward -936\n",
      "Epoch 65 Reward -45\n",
      "Epoch 10 Reward 55\n",
      "Epoch 78 Reward -166\n",
      "Epoch 287 Reward -654\n",
      "Epoch 178 Reward -302\n",
      "Epoch 71 Reward -168\n",
      "Epoch 65 Reward -126\n",
      "Epoch 273 Reward -667\n",
      "Epoch 422 Reward -789\n",
      "Epoch 26 Reward 48\n",
      "Epoch 271 Reward -476\n",
      "Epoch 52 Reward -23\n",
      "Epoch 171 Reward -241\n",
      "Epoch 128 Reward -180\n",
      "Epoch 144 Reward -277\n",
      "Epoch 55 Reward -62\n",
      "Epoch 37 Reward 46\n",
      "Epoch 40 Reward 7\n",
      "Epoch 8 Reward 75\n",
      "Epoch 152 Reward -393\n",
      "Epoch 53 Reward -24\n",
      "Epoch 13 Reward 79\n",
      "Epoch 182 Reward -288\n",
      "Epoch 197 Reward -303\n",
      "Epoch 15 Reward 68\n",
      "Epoch 192 Reward -325\n",
      "Epoch 97 Reward -167\n",
      "Epoch 139 Reward -245\n",
      "Epoch 23 Reward 51\n",
      "Epoch 19 Reward 28\n",
      "Epoch 207 Reward -349\n",
      "Epoch 87 Reward -103\n",
      "Epoch 13 Reward 52\n",
      "Epoch 2 Reward 90\n",
      "Epoch 61 Reward -23\n",
      "Epoch 60 Reward -67\n",
      "Epoch 31 Reward 43\n",
      "Epoch 11 Reward 81\n",
      "Epoch 119 Reward -162\n",
      "Epoch 63 Reward -52\n",
      "Epoch 39 Reward -10\n",
      "Epoch 70 Reward -23\n",
      "Epoch 216 Reward -466\n",
      "Epoch 219 Reward -352\n",
      "Epoch 307 Reward -566\n",
      "Epoch 117 Reward -88\n",
      "Epoch 23 Reward 60\n",
      "Epoch 588 Reward -1009\n",
      "Epoch 49 Reward -2\n",
      "Epoch 109 Reward -143\n",
      "Epoch 199 Reward -296\n",
      "Epoch 195 Reward -346\n",
      "Epoch 64 Reward -8\n",
      "Epoch 157 Reward -200\n",
      "Epoch 49 Reward -29\n",
      "Epoch 136 Reward -323\n",
      "Epoch 13 Reward 52\n",
      "Epoch 25 Reward -14\n",
      "Epoch 253 Reward -458\n",
      "Epoch 21 Reward 62\n",
      "Epoch 3 Reward 80\n",
      "Epoch 8 Reward 75\n",
      "Epoch 50 Reward 6\n",
      "Epoch 47 Reward -126\n",
      "Epoch 236 Reward -549\n",
      "Epoch 22 Reward 25\n",
      "Epoch 344 Reward -693\n",
      "Epoch 206 Reward -375\n",
      "Epoch 14 Reward 69\n",
      "Epoch 56 Reward -252\n",
      "Epoch 80 Reward -222\n",
      "Epoch 66 Reward -109\n",
      "Epoch 80 Reward -105\n",
      "Epoch 44 Reward 3\n",
      "Epoch 321 Reward -697\n",
      "Epoch 18 Reward 65\n",
      "Epoch 35 Reward 12\n",
      "Epoch 194 Reward -318\n",
      "Epoch 5 Reward 87\n",
      "Epoch 446 Reward -885\n",
      "Epoch 307 Reward -602\n",
      "Epoch 21 Reward 44\n",
      "Epoch 178 Reward -428\n",
      "Epoch 49 Reward -65\n",
      "Epoch 273 Reward -613\n",
      "Epoch 225 Reward -565\n",
      "Epoch 104 Reward -183\n",
      "Epoch 189 Reward -295\n",
      "Epoch 397 Reward -818\n",
      "Epoch 74 Reward -117\n",
      "Epoch 72 Reward -16\n",
      "Epoch 330 Reward -670\n",
      "Epoch 24 Reward 41\n",
      "Epoch 5 Reward 69\n",
      "Epoch 29 Reward 36\n",
      "Epoch 268 Reward -518\n",
      "Epoch 223 Reward -473\n",
      "Epoch 127 Reward -179\n",
      "Epoch 52 Reward 4\n",
      "Epoch 152 Reward -267\n",
      "Epoch 13 Reward 61\n",
      "Epoch 169 Reward -257\n",
      "Epoch 177 Reward -238\n",
      "Epoch 23 Reward -3\n",
      "Epoch 88 Reward -167\n",
      "Epoch 26 Reward 12\n",
      "Epoch 13 Reward 61\n",
      "Epoch 63 Reward -115\n",
      "Epoch 161 Reward -384\n",
      "Epoch 27 Reward 2\n",
      "Epoch 112 Reward -515\n",
      "Epoch 190 Reward -782\n",
      "Epoch 132 Reward -319\n",
      "Epoch 104 Reward -201\n",
      "Epoch 197 Reward -339\n",
      "Epoch 178 Reward -266\n",
      "Epoch 434 Reward -1197\n",
      "Epoch 73 Reward -143\n",
      "Epoch 643 Reward -1577\n",
      "Epoch 272 Reward -450\n",
      "Epoch 93 Reward -100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 Reward 41\n",
      "Epoch 82 Reward -44\n",
      "Epoch 222 Reward -472\n",
      "Epoch 143 Reward -285\n",
      "Epoch 12 Reward 71\n",
      "Epoch 54 Reward -52\n",
      "Epoch 366 Reward -679\n",
      "Epoch 99 Reward -106\n",
      "Epoch 122 Reward -219\n",
      "Epoch 292 Reward -479\n",
      "Epoch 14 Reward 87\n",
      "Epoch 48 Reward -37\n",
      "Epoch 33 Reward 41\n",
      "Epoch 26 Reward 39\n",
      "Epoch 168 Reward -202\n",
      "Epoch 16 Reward 58\n",
      "Epoch 445 Reward -839\n",
      "Epoch 196 Reward -338\n",
      "Epoch 101 Reward -135\n",
      "Epoch 643 Reward -1235\n",
      "Epoch 35 Reward 30\n",
      "Epoch 89 Reward -78\n",
      "Epoch 67 Reward -182\n",
      "Epoch 54 Reward -7\n",
      "Epoch 63 Reward -43\n",
      "Epoch 119 Reward -180\n",
      "Epoch 12 Reward 71\n",
      "Epoch 56 Reward -18\n",
      "Epoch 26 Reward 30\n",
      "Epoch 167 Reward -318\n",
      "Epoch 30 Reward 35\n",
      "Epoch 78 Reward -94\n",
      "Epoch 98 Reward -132\n",
      "Epoch 333 Reward -664\n",
      "Epoch 258 Reward -517\n",
      "Epoch 15 Reward 68\n",
      "Epoch 441 Reward -736\n",
      "Epoch 120 Reward -163\n",
      "Epoch 48 Reward 8\n",
      "Epoch 147 Reward -235\n",
      "Epoch 26 Reward 48\n",
      "Epoch 151 Reward -221\n",
      "Epoch 198 Reward -322\n",
      "Epoch 228 Reward -307\n",
      "Epoch 119 Reward -90\n",
      "Epoch 14 Reward 78\n",
      "Epoch 9 Reward 83\n",
      "Epoch 44 Reward -33\n",
      "Epoch 135 Reward -295\n",
      "Epoch 18 Reward 65\n",
      "Epoch 184 Reward -308\n",
      "Epoch 189 Reward -277\n",
      "Epoch 339 Reward -643\n",
      "Epoch 36 Reward -7\n",
      "Epoch 38 Reward 0\n",
      "Epoch 17 Reward 39\n",
      "Epoch 108 Reward -124\n",
      "Epoch 40 Reward -2\n",
      "Epoch 70 Reward -32\n",
      "Epoch 54 Reward -7\n",
      "Epoch 99 Reward -124\n",
      "Epoch 57 Reward -55\n",
      "Epoch 185 Reward -336\n",
      "Epoch 46 Reward -26\n",
      "Epoch 134 Reward -204\n",
      "Epoch 40 Reward -2\n",
      "Epoch 10 Reward 73\n",
      "Epoch 384 Reward -823\n",
      "Epoch 30 Reward 62\n",
      "Epoch 16 Reward 49\n",
      "Epoch 44 Reward -6\n",
      "Epoch 30 Reward 8\n",
      "Epoch 100 Reward -80\n",
      "Epoch 5 Reward 87\n",
      "Epoch 192 Reward -415\n",
      "Epoch 24 Reward 59\n",
      "Epoch 84 Reward -91\n",
      "Epoch 44 Reward 30\n",
      "Epoch 337 Reward -587\n",
      "Epoch 316 Reward -539\n",
      "Epoch 75 Reward -73\n",
      "Epoch 239 Reward -408\n",
      "Epoch 15 Reward 86\n",
      "Epoch 92 Reward -153\n",
      "Epoch 287 Reward -456\n",
      "Epoch 25 Reward 49\n",
      "Epoch 319 Reward -497\n",
      "Epoch 16 Reward 76\n",
      "Epoch 208 Reward -260\n",
      "Epoch 53 Reward -51\n",
      "Epoch 60 Reward -94\n",
      "Epoch 7 Reward 49\n",
      "Epoch 177 Reward -238\n",
      "Epoch 43 Reward 4\n",
      "Epoch 30 Reward 71\n",
      "Epoch 43 Reward 40\n",
      "Epoch 69 Reward -103\n",
      "Epoch 30 Reward -1\n",
      "Epoch 72 Reward -70\n",
      "Epoch 12 Reward 89\n",
      "Epoch 35 Reward -33\n",
      "Epoch 32 Reward 42\n",
      "Epoch 20 Reward 72\n",
      "Epoch 386 Reward -780\n",
      "Epoch 37 Reward -8\n",
      "Epoch 43 Reward 49\n",
      "Epoch 77 Reward -57\n",
      "Epoch 130 Reward -128\n",
      "Epoch 45 Reward 20\n",
      "Epoch 8 Reward 75\n",
      "Epoch 85 Reward -65\n",
      "Epoch 95 Reward -111\n",
      "Epoch 37 Reward 28\n",
      "Epoch 258 Reward -436\n",
      "Epoch 293 Reward -606\n",
      "Epoch 30 Reward 26\n",
      "Epoch 149 Reward -237\n",
      "Epoch 105 Reward -139\n",
      "Epoch 109 Reward -188\n",
      "Epoch 11 Reward 72\n",
      "Epoch 43 Reward 40\n",
      "Epoch 149 Reward -408\n",
      "Epoch 8 Reward 93\n",
      "Epoch 47 Reward -81\n",
      "Epoch 16 Reward 13\n",
      "Epoch 384 Reward -1831\n",
      "Epoch 324 Reward -709\n",
      "Epoch 109 Reward -152\n",
      "Epoch 40 Reward -38\n",
      "Epoch 115 Reward -158\n",
      "Epoch 159 Reward -238\n",
      "Epoch 42 Reward -4\n",
      "Epoch 27 Reward 47\n",
      "Epoch 62 Reward -60\n",
      "Epoch 25 Reward 49\n",
      "Epoch 37 Reward 19\n",
      "Epoch 120 Reward -109\n",
      "Epoch 87 Reward -121\n",
      "Epoch 348 Reward -1030\n",
      "Epoch 15 Reward 68\n",
      "Epoch 318 Reward -667\n",
      "Epoch 155 Reward -216\n",
      "Epoch 31 Reward 43\n",
      "Epoch 42 Reward 5\n",
      "Epoch 60 Reward -67\n",
      "Epoch 24 Reward 41\n",
      "Epoch 327 Reward -577\n",
      "Epoch 26 Reward 21\n",
      "Epoch 59 Reward -39\n",
      "Epoch 87 Reward -103\n",
      "Epoch 20 Reward 54\n",
      "Epoch 191 Reward -342\n",
      "Epoch 131 Reward -219\n",
      "Epoch 11 Reward 63\n",
      "Epoch 74 Reward -45\n",
      "Epoch 75 Reward -109\n",
      "Epoch 97 Reward -77\n",
      "Epoch 38 Reward -18\n",
      "Epoch 45 Reward 2\n",
      "Epoch 18 Reward 65\n",
      "Epoch 46 Reward -26\n",
      "Epoch 52 Reward -14\n",
      "Epoch 51 Reward -22\n",
      "Epoch 128 Reward -171\n",
      "Epoch 12 Reward 89\n",
      "Epoch 123 Reward -193\n",
      "Epoch 51 Reward -22\n"
     ]
    }
   ],
   "source": [
    "# tensorflow와 그 안의 모델을 실행\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    # 로그를 기록할 파일 열기\n",
    "    log_file = \"log.txt\"\n",
    "    \n",
    "    #에피소드\n",
    "    for i in range(num_episodes):\n",
    "        done = False\n",
    "        obs = env.reset()\n",
    "        epoch = 0\n",
    "        episodic_reward = 0\n",
    "        actions_counter = Counter()\n",
    "        episodic_loss = []\n",
    "        \n",
    "        #상태가 최종 상태가 아닌 동안\n",
    "        while not done:\n",
    "            # 전처리된 게임 화면 가져오기\n",
    "            obs = env.render('dqn')\n",
    "            \n",
    "            # 게임 화면을 피드하고 각 작업에 대한 Q 값을 가져오기\n",
    "            actions = mainQ_outputs.eval(feed_dict={X:[obs], in_training_mode: False})\n",
    "            \n",
    "            # 행동 가져오기\n",
    "            action = np.argmax(actions, axis =-1)\n",
    "            actions_counter[str(action)] += 1\n",
    "            \n",
    "            #엡실론 그리디 정책을 사용하여 행동 선택\n",
    "            action = epsilon_greedy(action, global_step)\n",
    "            \n",
    "            #행동을 수행하고 다음 상태인 next_obs로 이동하여 보상을 받는다.\n",
    "            next_obs, reward, done, _ = env.step(action)\n",
    "            \n",
    "            #이 전환을 재생 버퍼에 경험으로 저장\n",
    "            exp_buffer.append([obs, action, env.render('dqn'), reward, done])\n",
    "            \n",
    "            #특정 단계 후에 경험 버퍼의 샘플로 Q 네트워크를 훈련\n",
    "            if global_step % steps_train == 0 and global_step > start_steps:\n",
    "                # 샘플 경험\n",
    "                o_obs, o_act, o_next_obs, o_rew, o_done = sample_memories(batch_size)\n",
    "                \n",
    "                # 상태\n",
    "                o_obs = [x for x in o_obs]\n",
    "                \n",
    "                # 다음 상태\n",
    "                o_next_obs = [x for x in o_next_obs]\n",
    "                \n",
    "                # 다음 행동\n",
    "                next_act = mainQ_outputs.eval(feed_dict={X:o_next_obs, in_training_mode:False})\n",
    "                \n",
    "                # 보상\n",
    "                y_batch = o_rew + discount_factor * np.max(next_act, axis=-1) * (1-o_done)\n",
    "                \n",
    "                # 모든 요약을 병합하고 파일에 쓰기\n",
    "                mrg_summary = merge_summary.eval(feed_dict={X: o_obs, y:np.expand_dims(y_batch, axis=-1), X_action:o_act, in_training_mode:False})\n",
    "                file_writer.add_summary(mrg_summary, global_step)\n",
    "                \n",
    "                # 네트워크 훈련 및 loss 계산\n",
    "                train_loss, _ = sess.run([loss, training_op], feed_dict={X:o_obs, y:np.expand_dims(y_batch, axis=-1), X_action:o_act, in_training_mode: True})\n",
    "                episodic_loss.append(train_loss)\n",
    "                \n",
    "            # 일정 간격 후에 주요 Q 네트워크 가중치를 대상 Q 네트워크에 복사\n",
    "            if (global_step+1)%copy_steps == 0 and global_step > start_steps:\n",
    "                copy_target_to_main.run()\n",
    "                    \n",
    "            epoch += 1\n",
    "            global_step += 1\n",
    "            episodic_reward += reward\n",
    "            \n",
    "        # 파일에 로그 기록\n",
    "        with open(log_file, \"a\") as file:\n",
    "            file.write(\"Epoch: \" + str(epoch) + \", Reward: \" + str(episodic_reward))\n",
    "            file.write(\", Loss: \" + str(np.mean(episodic_loss)))\n",
    "            file.write(\", Actions: \" + str(actions_counter) + \"\\n\")\n",
    "        \n",
    "        # 정보 출력\n",
    "        print('Epoch', epoch, 'Reward', episodic_reward)\n",
    "        \n",
    "    # 저장 객체 생성\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    # 모델 저장\n",
    "    saver.save(sess, './model', global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fffa7285be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o - - - - - - - - - - - - - - - - - o\n",
      "| P |       |           |           |\n",
      "|           | - |   |   | - | - |   |\n",
      "|   |     2         |   |           |\n",
      "| - |                       | - |   |\n",
      "|       |   |                       |\n",
      "|   |                       | - |   |\n",
      "|   |                       |       |\n",
      "|   | - | - | - |           | - |   |\n",
      "|                         0     | - |\n",
      "o - - - - - - - - - - - - - - - - - o\n",
      "\n",
      "Episode Reward=  -380\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-b0e96d4e997e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Episode Reward= \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fffa7285be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tensorflow와 그 안의 모델을 실행\n",
    "with tf.Session() as sess:\n",
    "    # 저장 객체 생성\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    # 저장된 모델 로드\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('./')) \n",
    "\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "\n",
    "    #상태가 최종 상태가 아닌 동안\n",
    "    while not done:\n",
    "\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "        env.render()\n",
    "\n",
    "        # 전처리된 게임 화면 가져오기\n",
    "        obs = env.render('dqn')\n",
    "\n",
    "        # 게임 화면을 피드하고 각 작업에 대한 Q 값을 가져오기\n",
    "        actions = mainQ_outputs.eval(feed_dict={X:[obs], in_training_mode: False})\n",
    "\n",
    "        # 행동 가져오기\n",
    "        action = np.argmax(actions, axis =-1)\n",
    "\n",
    "        #행동을 수행하고 다음 상태인 next_obs로 이동하여 보상을 받는다.\n",
    "        next_obs, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        print(\"Episode Reward= \", total_reward)\n",
    "        time.sleep(0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
